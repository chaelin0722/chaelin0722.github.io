---
title:  "[ë…¼ë¬¸ì •ë¦¬ðŸ“ƒ] ImageNet Classification with Deep Convolutional Neural Networks"
excerpt: "Week2 -AlexNet-"

categories:
  - CNN
  - paperReview
tags: [CNN, paperReview]

last_modified_at: 2021-06-10T08:06:00-05:00
classes: wide
---


[ë…¼ë¬¸ì›ë³¸](https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)ðŸ˜™

2012 ILSVRCì—ì„œ ìˆ˜ìƒí•œ neural network **"AlexNet"**

AlexNet ì½”ë“œêµ¬í˜„ íŽ˜ì´ì§€. => [AlexNet](https://chaelin0722.github.io/deeplearning/cnn/code/AlexNet_code/)


## 0. ìš”ì•½

ILSVRC-2010 ì½˜í…ŒìŠ¤íŠ¸ì—ì„œ ImageNetì˜ 120ë§Œ ê³ í™”ì§ˆ ì´ë¯¸ì§€ë¥¼ ë¶„ë¥˜í•˜ê¸° ìœ„í•´ í¬ê³  ê¹Šì€ CNNì„ ì‚¬ìš©í•´ 1000ê°œì˜ ë‹¤ë¥¸ í´ëž˜ìŠ¤ë“¤ì„ ë„ì¶œí•˜ì˜€ë‹¤.

60million íŒŒë¼ë¯¸í„°ì™€ 650000 neurons, 5ê°œì˜ convolutional layers,  max-pooling layers ê·¸ë¦¬ê³  3ê°œì˜ fully-connected layers

- To train faster â‡’ use non-saturating neurons and íš¨ìœ¨ì ì¸ GPU
- To reduce â‡’ use dropout, which is a recently-developed regularization method

top-5 test error rate of 15.3%ë¥¼ ê¸°ë¡í•˜ë©° ILSVRC-2012 ëŒ€íšŒì—ì„œ 1ë“±ì„ ìˆ˜ìƒí•˜ì˜€ë‹¤.




## 1. ê°œìš”

ì´ì „ì—ëŠ” ë ˆì´ë¸”ëœ ì´ë¯¸ì§€ ë°ì´í„°ì…‹ì€ ë§¤ìš° ì ì—ˆë‹¤. ë˜í•œ ì´ëŸ° ë°ì´í„°ì…‹ì€ ë‹¨ìˆœì¸ì‹ ë¬¸ì œì—ì„œë§Œ ìž˜ ì ìš©ì´ ë˜ì—ˆë‹¤. í•˜ì§€ë§Œ ì§€ê¸ˆì€ ìˆ˜ë°±ë§Œê°œì˜ ì´ë¯¸ì§€ë¡œ ë ˆì´ë¸”ë§ ëœ ë°ì´í„° ì…‹ì„ ìˆ˜ì§‘ ê°€ëŠ¥í•˜ë‹¤.

í•˜ì§€ë§Œ,

ì—„ì²­ ë³µìž¡í•œ ì‚¬ë¬¼ ì¸ì‹ ë¬¸ì œëŠ” ImageNetë§Œí¼ í° ë°ì´í„°ì…‹ì—ë„ ë¶ˆêµ¬í•˜ê³  êµ¬ì²´í™” ë˜ì§€ ì•ŠëŠ”ë‹¤.

ë”°ë¼ì„œ ëª¨ë¸ì€ ë§Žì€ ì‚¬ì „ ì§€ì‹ê³¼ ì•„ì§ ê°€ì§€ê³  ìžˆì§€ ì•Šì€ ëª¨ë“  ë°ì´í„°(í•™ìŠµë˜ì§€ ì•Šì€ ë°ì´í„°)ë¥¼ ì´í•´í•´ì•¼ í•œë‹¤.

CNNì€ ê¹Šì´ì™€ ë„“ì´ë¥¼ ë‹¤ì–‘í•˜ê²Œ ì»¨íŠ¸ë¡¤í•  ìˆ˜ ìžˆê³  ê°•í•˜ê²Œë„ ë§Œë“¤ ìˆ˜ ìžˆë‹¤. ë˜í•œ, ìžì—°ì˜ ì´ë¯¸ì§€ì— ëŒ€í•´ì„œ ê±°ì˜ ëŒ€ë¶€ë¶„ ë§žëŠ” ì˜ˆì¸¡ì„ í•œë‹¤. ë”°ë¼ì„œ ê¸°ì¡´ì˜ feed-forward neural networksì™€ ë¹„êµí–ˆì„ ë•Œ CNNì€ ë” ì ì€ ì—°ê²°ê³¼ íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì§€ê³  ìžˆì–´ ì‰½ê²Œ í•™ìŠµí•  ìˆ˜ ìžˆìœ¼ë©° ë™ì‹œì— ì´ë¡ ì ìœ¼ë¡œ ì•„ì£¼ ë¯¸ë¯¸í•˜ê²Œ ë‚˜ì˜ë©° ìµœê³ ì˜ ì„±ëŠ¥ì„ ë‚¸ë‹¤

1) CNNì˜ ì§ˆê³¼ 2)ì§€ì—­ì  êµ¬ì¡°ì˜ ìƒëŒ€ì  íš¨ìœ¨ì—ë„ ë¶ˆêµ¬í•˜ê³  í° ìŠ¤ì¼€ì¼ì—ì„œ ê³  í™”ì§ˆì„ ì ìš©í•˜ê¸°ì—ëŠ” ì—¬ì „ížˆ ë¹„ìš©ì´ ê½¤ ë“ ë‹¤. ë‹¤í–‰ížˆ í˜„ìž¬ GPU(highly-optimized implementation of 2D convolutionìœ¼ë¡œ ìŒì„ ì´ë£¨ëŠ” GPU)ë¡œ ì„±ëŠ¥ì„ ë‚¼ ìˆ˜ ìžˆë‹¤.

ìš°ë¦¬ëŠ” ì´ ëª¨ë¸ì„ í†µí•´ì„œ ì„±ëŠ¥ì„ ë†’ì´ê³  í•™ìŠµ ì‹œê°„ì„ ì¤„ì˜€ë‹¤. ìš°ë¦¬ì˜ ì‹¤í—˜ì€ ì„±ëŠ¥ì´ ë” ì¢‹ì€ GPUê°€ ë°œëª…ë¨ìœ¼ë¡œì¨ ë” í–¥ìƒë  ìˆ˜ ìžˆë‹¤.




## 2. Dataset

ImageNetì„ ì‚¬ìš©, ì´ ì‹œìŠ¤í…œì—ì„œëŠ” ì¼ì •í•œ ìž…ë ¥ dimensionê°’ì´ í•„ìš”í•˜ë¯€ë¡œ ê°€ë³€í•´ìƒë„ ì´ë¯¸ì§€ë¡œ êµ¬ì„±ëœ ImageNet ë°ì´í„°ë“¤ì„ 256ì˜ í•´ìƒë„ë¡œ ë‹¤ìš´ ìƒ˜í”Œë§í•˜ì˜€ë‹¤. ë˜, ì§ì‚¬ê°í˜• ì´ë¯¸ì§€ê°€ ìž…ë ¥ìœ¼ë¡œ ë“¤ì–´ì˜¬ ê²½ìš° ê¸¸ì´ê°€ ì§§ì€ ìª½ì— rescaleì„ ì§„í–‰í•´ ê°€ìš´ë°ë¡œ ë§žì¶”ê³  ì§„í–‰í•˜ì˜€ë‹¤. ê° í”½ì…€ì—ì„œ í›ˆë ¨ setì— ëŒ€í•œ í‰ê· ì„ ë¹¼ëŠ” ë°©ë²•ìœ¼ë¡œë§Œ ì´ë¯¸ì§€ë¥¼ ì‚¬ì „ì²˜ë¦¬í•˜ì˜€ë‹¤. ë”°ë¼ì„œ ì´ ëª¨ë¸ì—ì„œëŠ” raw RGB ê°’ì˜ í”½ì…€ë¡œ í•™ìŠµì„ ì§„í–‰í•˜ì˜€ë‹¤.





## 3. The Architecture

![image](https://user-images.githubusercontent.com/53431568/119877176-a7a1d300-bf63-11eb-8839-061d7750c517.png)

ì´ 8ê°œì˜ layerë¡œ, 5ê°œì˜ convolutional layer, 3ê°œì˜ fully-connected layerë¡œ êµ¬ì„±ë˜ì–´ìžˆë‹¤.

### 3.1 ReLU nonlinearity

ê°œë… ì •ë¦¬ ë¨¼ì €!

- saturating linearity : ì–´ë–¤ ìž…ë ¥ xê°€ ë¬´í•œëŒ€ë¡œ ê°ˆ ë•Œ í•¨ìˆ˜ ê°’ì´ ë¬´í•œëŒ€ë¡œ ê°€ëŠ” ê²ƒ ex) LeRu
- non-saturating linearity : ì–´ë–¤ ìž…ë ¥ xê°€ ë¬´í•œëŒ€ë¡œ ê°ˆ ë•Œ í•¨ìˆ˜ ê°’ì´ ì–´ë–¤ ë²”ìœ„ ë‚´ì—ì„œë§Œ ì›€ì§ì´ëŠ” ê²ƒ ex) Sigmoid, hyper tahn

![image](https://user-images.githubusercontent.com/53431568/119877228-bab4a300-bf63-11eb-8304-59a2175bc694.png)

ìœ„ì˜ í‘œë¥¼ ë³´ë©´, ì‹¤ì„ ì¸ ReLu í•¨ìˆ˜ê°€ ì ì„   tanh í•¨ìˆ˜ë³´íƒ€ ì•½ 6ë°° ë¹ ë¥´ê²Œ í•™ìŠµí•  ìˆ˜ ìžˆë‹¤ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìžˆë‹¤. ì´ê²ƒì€, ì „í†µëª¨ë¸ì„ ì‚¬ìš©í–ˆë”ë¼ë©´ ì´ ìž‘ì—…ì— ëŒ€í•´ í° neural net ì‹¤í—˜ì´ ë¶ˆê°€ëŠ¥í–ˆìŒì„ ë³´ì—¬ì¤€ë‹¤.

**ë¹ ë¥¸ í•™ìŠµì€ í° ê·œëª¨ì˜ ë°ì´í„°ì…‹ìœ¼ë¡œ ì´ë£¨ì–´ì§„ í° ëª¨ë¸ì˜ ì„±ëŠ¥ì— ì¢‹ì€ ì˜í–¥ì„ ë¯¸ì¹œë‹¤.**

### 3.2 Training on Multiple GPUs

120ë§Œì˜ í•™ìŠµ ì˜ˆì œëŠ” í•˜ë‚˜ì˜ GPUë¡œëŠ” ë„ˆë¬´ í¬ë‹¤. ë”°ë¼ì„œ ë‘ê°œì˜ GPUë¥¼ íŽ¼ì³ì„œ ë§(net)ì„ ì—‡ê°ˆë¦¬ê²Œ í•˜ì˜€ë‹¤. 


![image](https://user-images.githubusercontent.com/53431568/119877274-ca33ec00-bf63-11eb-98a6-5f016782877b.png)

ìœ„ì˜ ì´ë¯¸ì§€ì™€ ê°™ì´ kernel 2ê°œë¥¼ ë™ì‹œì— ì‚¬ìš©(GPU 2ê°œë¡œ ë¶„í• í•´ì„œ) ë˜ ì´ê²ƒì„ crossí•˜ê²Œ í•™ìŠµí•˜ê²Œ ë” í•˜ì˜€ë‹¤. 

- ìµœê·¼ì˜ GPUë“¤ì€ êµì°¨ ë³‘ë ¬ì²˜ë¦¬ê°€ ìž˜ ë˜ì–´ìžˆì–´ í˜¸ìŠ¤íŠ¸ ë¨¸ì‹  ë©”ëª¨ë¦¬ì— ì§ì ‘ ê°€ì§€ ì•Šê³ ë„ ì„œë¡œì˜ ë©”ëª¨ë¦¬ë¥¼ R/W í•  ìˆ˜ ìžˆë‹¤.
- ê° ì»¤ë„ì€ ë°”ë¡œ ì•ž ë‹¨ layerì—ì„œ ë§Œ ìž…ë ¥ì„ ë°›ëŠ”ë‹¤.
- ì—°ê²°ì˜ íŒ¨í„´ì„ ê³ ë¥´ëŠ” ê²ƒì€ cross-validationì˜ ë¬¸ì œì´ì§€ë§Œ ì´ê²ƒ ë•ë¶„ì— í—ˆìš© ê°€ëŠ¥í•œ ê³„ì‚° ê°’ì˜ ë¶„ìˆ˜ê°€ ë‚˜ì˜¬ ë•Œê¹Œì§€ ë¯¸ì„¸ ì¡°ì •ì´ ê°€ëŠ¥í•˜ë‹¤.

### 3.3 Local Response Normalization

tahn ì´ë‚˜ sigmoid í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë©´ saturatingì„ í•´ì•¼í•´ì„œ overfittingì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ìž…ë ¥í• ë•Œ ë¶€í„° normalizationì„ ë“¤ì–´ê°„ë‹¤. í•˜ì§€ë§Œ ReLuëŠ” ì–‘ìˆ˜ ìž…ë ¥ê°’ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— convolution ì´ë‚˜ poolingì‹œ ë§¤ìš° ë†’ì€ í•˜ë‚˜ì˜ í”½ì…€ê°’ì´ ì£¼ë³€ì˜ í”½ì…€ì— ì˜í–¥ì„ ë¯¸ì¹˜ê²Œ ëœë‹¤. ì´ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ë‹¤ë¥¸ activation map(kernel)ì˜ ê°™ì€ ìœ„ì¹˜ì— ìžˆëŠ” í”½ì…€ë¼ë¦¬ normalizationì„ í•´ì£¼ëŠ”ë° ì´ê²ƒì´ Local Response Normalizationì´ë‹¤.

- ì´ ì •ê·œí™”ëŠ” top-1, top-5 ì—ëŸ¬ë¹„ìœ¨ì„ 1.4%ì™€ 1.2% ë¡œ ì¤„ì˜€ë‹¤.
- 1,2 layerì— ì‚¬ìš©í•˜ë©° ReLU í•¨ìˆ˜ ì ìš© í›„ì— ì‚¬ìš©í•œë‹¤.

### 3.4 Overlapping Pooling

poolingì€ íŠ¹ì„±ë§µ(feature map)ì„ ì¤„ì´ê¸° ìœ„í•´ ì‚¬ìš©í•˜ëŠ” ê²ƒ. ì—¬ê¸°ì„œëŠ” overlapping, max pooling ì„ ì‚¬ìš©í•œë‹¤. â‡’ ê²°ê³¼ë¡œ overfitì´ ìž˜ ë‚˜ì˜¤ì§€ ì•ŠëŠ” ê²ƒì„ ë°œê²¬

- Non-overlappingê³¼ ë¹„êµí–ˆì„ ë•Œ top-1, top-5 ì—ëŸ¬ë¹„ìœ¨ì„ 0.4%ì™€ 0.3% ê¹Œì§€ ì¤„ì˜€ë‹¤.

![image](https://user-images.githubusercontent.com/53431568/119877326-dc158f00-bf63-11eb-9bb1-538ec3f1eb32.png)

ëŒ€í‘œì ìœ¼ë¡œ Lenet-5ëŠ” non-overlapping, average poolingì„ ì‚¬ìš©í•˜ì˜€ë‹¤. 

### 3.5 Overall Archtecture

![image](https://user-images.githubusercontent.com/53431568/119877176-a7a1d300-bf63-11eb-8839-061d7750c517.png)

AlexNetì€ ì´ 8ê°œì˜ layerë¡œ, 5ê°œì˜ convolutional layer, 3ê°œì˜ fully-connected layerë¡œ êµ¬ì„±ë˜ì–´ìžˆë‹¤. 2,3,5 ë²ˆì§¸ convolution layerì€ ì „ ë‹¨ê³„ì˜ ê°™ì€ ì±„ë…ˆì˜ íŠ¹ì„±ë§µë“¤ê³¼ ì—°ê²°ë˜ì–´ìžˆëŠ” ë°˜ë©´, 3ë²ˆì§¸ convolution layerì€ ì „ ë‹¨ê³„ì˜ ë‘ ì±„ë„ì˜ íŠ¹ì„± ë§µë“¤ê³¼ ëª¨ë‘ ì—°ê²°ë˜ì–´ ìžˆë‹¤. ëª¨ë“  layerëŠ” ReLUí•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ê³  ë§ˆì§€ë§‰ outputì—ë§Œ softmaxí•¨ìˆ˜ë¥¼ ì ìš©í•œë‹¤.

ì´ì œ ê° ë ˆì´ì–´ë§ˆë‹¤ ì–´ë–¤ ìž‘ì—…ì´ ìˆ˜í–‰ë˜ëŠ”ì§€ ì‚´íŽ´ë³´ìž. ìš°ì„  AlexNetì— ìž…ë ¥ ë˜ëŠ” ê²ƒì€ 227 x 227 x 3 ì´ë¯¸ì§€ë‹¤. (227 x 227 ì‚¬ì´ì¦ˆì˜ RGB ì»¬ëŸ¬ ì´ë¯¸ì§€) 

**1) ì²«ë²ˆì§¸ ë ˆì´ì–´(ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´):**Â 96ê°œì˜Â 11 x 11 x 3 ì‚¬ì´ì¦ˆÂ ì»¤ë„ë¡œ ìž…ë ¥ ì˜ìƒì„ ì»¨ë³¼ë£¨ì…˜ í•´ì¤€ë‹¤. ì»¨ë³¼ë£¨ì…˜ ë³´í­(stride)ë¥¼ 4ë¡œ ì„¤ì •í–ˆê³ , zero-paddingì€ ì‚¬ìš©í•˜ì§€ ì•Šì•˜ë‹¤. zero-paddingì€ ì»¨ë³¼ë£¨ì…˜ìœ¼ë¡œ ì¸í•´ íŠ¹ì„±ë§µì˜ ì‚¬ì´ì¦ˆê°€ ì¶•ì†Œë˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´, ë˜ëŠ” ì¶•ì†Œë˜ëŠ” ì •ë„ë¥¼ ì¤„ì´ê¸° ìœ„í•´ ì˜ìƒì˜ ê°€ìž¥ìžë¦¬ ë¶€ë¶„ì— 0ì„ ì¶”ê°€í•˜ëŠ” ê²ƒì´ë‹¤. ê²°ê³¼ì ìœ¼ë¡œ 55 x 55 x 96Â íŠ¹ì„±ë§µ(96ìž¥ì˜ 55 x 55 ì‚¬ì´ì¦ˆ íŠ¹ì„±ë§µë“¤)ì´ ì‚°ì¶œëœë‹¤. ê·¸ ë‹¤ìŒì— ReLU í•¨ìˆ˜ë¡œ í™œì„±í™”í•´ì¤€ë‹¤.Â ì´ì–´ì„œ 3 x 3 overlapping max poolingì´ stride 2ë¡œ ì‹œí–‰ëœë‹¤. ê·¸ ê²°ê³¼ 27 x 27Â x 96 íŠ¹ì„±ë§µì„ ê°–ê²Œ ëœë‹¤. ê·¸ ë‹¤ìŒì—ëŠ” ìˆ˜ë ´ ì†ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ local response normalizationì´ ì‹œí–‰ëœë‹¤. local response normalizationì€ íŠ¹ì„±ë§µì˜ ì°¨ì›ì„ ë³€í™”ì‹œí‚¤ì§€ ì•Šìœ¼ë¯€ë¡œ, íŠ¹ì„±ë§µì˜ í¬ê¸°ëŠ” 27 x 27 x 96ìœ¼ë¡œ ìœ ì§€ëœë‹¤.

**2) ë‘ë²ˆì§¸ ë ˆì´ì–´(ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´):**Â 256ê°œì˜ 5 x 5 x 48Â ì»¤ë„ì„ ì‚¬ìš©í•˜ì—¬ ì „ ë‹¨ê³„ì˜ íŠ¹ì„±ë§µì„ ì»¨ë³¼ë£¨ì…˜í•´ì¤€ë‹¤. strideëŠ” 1ë¡œ, zero-paddingì€ 2ë¡œ ì„¤ì •í–ˆë‹¤. ë”°ë¼ì„œ 27 x 27 x 256 íŠ¹ì„±ë§µ(256ìž¥ì˜ 27 x 27 ì‚¬ì´ì¦ˆ íŠ¹ì„±ë§µë“¤)ì„ ì–»ê²Œ ëœë‹¤. ì—­ì‹œ ReLU í•¨ìˆ˜ë¡œ í™œì„±í™”í•œë‹¤. ê·¸ ë‹¤ìŒì— 3 x 3 overlapping max poolingì„ stride 2ë¡œ ì‹œí–‰í•œë‹¤. ê·¸ ê²°ê³¼ 13 x 13 x 256 íŠ¹ì„±ë§µì„ ì–»ê²Œ ëœë‹¤. ê·¸ í›„ local response normalizationì´ ì‹œí–‰ë˜ê³ , íŠ¹ì„±ë§µì˜ í¬ê¸°ëŠ” 13 x 13 x 256ìœ¼ë¡œ ê·¸ëŒ€ë¡œ ìœ ì§€ëœë‹¤.

**3) ì„¸ë²ˆì§¸ ë ˆì´ì–´(ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´):**Â 384ê°œì˜ 3 x 3 x 256 ì»¤ë„ì„ ì‚¬ìš©í•˜ì—¬Â ì „ ë‹¨ê³„ì˜ íŠ¹ì„±ë§µì„Â ì»¨ë³¼ë£¨ì…˜í•´ì¤€ë‹¤. strideì™€ zero-padding ëª¨ë‘ 1ë¡œ ì„¤ì •í•œë‹¤. ë”°ë¼ì„œ 13 x 13 x 384 íŠ¹ì„±ë§µ(384ìž¥ì˜ 13 x 13 ì‚¬ì´ì¦ˆ íŠ¹ì„±ë§µë“¤)ì„ ì–»ê²Œ ëœë‹¤.Â 

**4) ë„¤ë²ˆì§¸ ë ˆì´ì–´(ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´):**Â 384ê°œì˜ 3 x 3 x 192 ì»¤ë„ì„ ì‚¬ìš©í•´ì„œÂ ì „ ë‹¨ê³„ì˜ íŠ¹ì„±ë§µì„Â ì»¨ë³¼ë£¨ì…˜í•´ì¤€ë‹¤. strideì™€ zero-padding ëª¨ë‘ 1ë¡œ ì„¤ì •í•œë‹¤. ë”°ë¼ì„œ 13 x 13 x 384 íŠ¹ì„±ë§µ(384ìž¥ì˜ 13 x 13 ì‚¬ì´ì¦ˆ íŠ¹ì„±ë§µë“¤)ì„ ì–»ê²Œ ëœë‹¤.Â 

**5) ë‹¤ì„¯ë²ˆì§¸ ë ˆì´ì–´(ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´):**Â 256ê°œì˜ 3 x 3 x 192 ì»¤ë„ì„ ì‚¬ìš©í•´ì„œÂ ì „ ë‹¨ê³„ì˜ íŠ¹ì„±ë§µì„Â ì»¨ë³¼ë£¨ì…˜í•´ì¤€ë‹¤. strideì™€ zero-padding ëª¨ë‘ 1ë¡œ ì„¤ì •í•œë‹¤. ë”°ë¼ì„œ 13 x 13 x 256 íŠ¹ì„±ë§µ(256ìž¥ì˜ 13 x 13 ì‚¬ì´ì¦ˆ íŠ¹ì„±ë§µë“¤)ì„ ì–»ê²Œ ëœë‹¤.Â ReLUë¡œ í™œì„±í™” í›„ 3 x 3 overlapping max poolingì„ stride 2ë¡œ ì‹œí–‰í•œë‹¤. ê·¸ ê²°ê³¼ 6 x 6 x 256 íŠ¹ì„±ë§µì„ ì–»ê²Œ ëœë‹¤.

**6) ì—¬ì„¯ë²ˆì§¸ ë ˆì´ì–´(Fully connected layer):**Â 6 x 6 x 256 íŠ¹ì„±ë§µì„ flattení•´ì¤˜ì„œ 6 x 6 x 256 = 9216ì°¨ì›ì˜ ë²¡í„°ë¡œ ë§Œë“¤ì–´ì¤€ë‹¤. ê·¸ê²ƒì„ ì—¬ì„¯ë²ˆì§¸ ë ˆì´ì–´ì˜ 4096ê°œì˜ ë‰´ëŸ°ê³¼ fully connected í•´ì¤€ë‹¤. ê·¸ ê²°ê³¼ë¥¼Â ReLU í•¨ìˆ˜ë¡œ í™œì„±í™”í•œë‹¤.

**7) ì¼ê³±ë²ˆì§¸ ë ˆì´ì–´(Fully connected layer):**Â 4096ê°œì˜ ë‰´ëŸ°ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìžˆë‹¤. ì „ ë‹¨ê³„ì˜ 4096ê°œ ë‰´ëŸ°ê³¼ fully connectedë˜ì–´ ìžˆë‹¤. 

**8) ì—¬ëŸë²ˆì§¸ ë ˆì´ì–´(Fully connected layer):**Â 1000ê°œì˜ ë‰´ëŸ°ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìžˆë‹¤. ì „ ë‹¨ê³„ì˜ 4096ê°œ ë‰´ëŸ°ê³¼ fully connectedë˜ì–´ ìžˆë‹¤. 1000ê°œ ë‰´ëŸ°ì˜ ì¶œë ¥ê°’ì—Â softmax í•¨ìˆ˜ë¥¼ ì ìš©í•´ 1000ê°œ í´ëž˜ìŠ¤ ê°ê°ì— ì†í•  í™•ë¥ ì„ ë‚˜íƒ€ë‚¸ë‹¤.




## 4. Reducing Overfitting

ì´ NNì˜ êµ¬ì¡°ëŠ” 6000ë§Œê°œì˜ íŒŒë¼ë¯¸í„°ë¥¼ ê°–ê³ ìžˆë‹¤. 1000ê°œ ILSVRC í´ëž˜ìŠ¤ë“¤ë¡œ ì¸í•´ ê° í•™ìŠµ ì˜ˆì œê°€ ì´ë¯¸ì§€ë¶€í„° ë ˆì´ë¸”ê¹Œì§€ ë§¤í•‘í•  ë•Œ 10bitë¡œ ì œì–´ë¥¼ í•´ë„ overfittingì— ëŒ€í•œ ê³ ë ¤ ì—†ì´ ì•„ì£¼ ë§Žì€ íŒŒë¼ë¯¸í„°ë¥¼ í•™ìŠµì‹œí‚¤ê¸°ì—ëŠ” ë¶ˆì¶©ë¶„í•œ ê²ƒìœ¼ë¡œ ë°í˜€ì¡Œë‹¤. ë‘ ê°€ì§€ ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

### 4.1 Data Augmentation

overfittingì„ ë°©ì§€í•˜ê¸° ìœ„í•œ ì•„ì£¼ ê¸°ë³¸ì ì¸ ë°©ë²•ì€ ë ˆì´ë¸”ì„ ë³´ì¡´í•˜ë©´ì„œ ë³€í™˜ì‹œí‚¤ëŠ” ë°©ë²•ìœ¼ë¡œ ë°ì´í„°ì…‹ì„ ì¸ê³µì ìœ¼ë¡œ ëŠ˜ë¦¬ëŠ” ê²ƒì´ë‹¤.

  1) ì´ë¯¸ì§€ ë²ˆì—­ê³¼ ìˆ˜í‰ ë°˜ì „

  2) RGB í”½ì…€ê°’ì˜ ì§‘í•©ì— PCA ì ìš©

### 4.2 Dropout

ì˜ˆì¸¡ì„ ì¡°í•©í•˜ëŠ” ê²ƒì€ ì¢‹ì€ ë°©ë²•ì´ë‚˜ ì˜¤ëžœ ì‹œê°„ì´ ê±¸ë¦¬ê³  ì•„ì£¼ í° NNì— ë†’ì€ ë¹„ìš©ì´ ë°œìƒí•œë‹¤. ë”°ë¼ì„œ dropoutìœ¼ë¡œ ë‰´ëŸ°ë“¤ê°„ì˜ ë³µìž¡í•œ ìƒí˜¸ ì˜ì¡´ë„ë¥¼ ë‚®ì¶”ì–´ì„œ overfittingì„ ë§‰ì„ ìˆ˜ ìžˆë‹¤.  

- "dropout"ì´ë¼ëŠ” ê²ƒì€ forward passì— ê´€ì—¬í•˜ì§€ ì•Šê³  back propagationì—ë„ ì°¸ì—¬í•˜ì§€ ì•ŠëŠ”ê²ƒì´ë‹¤. ë‹¨, trainingí•  ë•Œë§Œ ì°¸ì—¬í•˜ì§€ ì•Šê³  testí•  ë•ŒëŠ” ì°¸ì—¬í•´ì•¼ í•œë‹¤.

![image](https://user-images.githubusercontent.com/53431568/119877421-fbacb780-bf63-11eb-806b-0eac7302d14f.png)




## 5. Details of learning

ê·¸ëƒ¥ ë…¼ë¬¸ ì°¸ê³ í•´ë³´ê¸°.. 


## 6. Results

![image](https://user-images.githubusercontent.com/53431568/119877472-0c5d2d80-bf64-11eb-8374-b46819fd1433.png)

table2ì—ì„œ ë³´ëŠ” ë°”ì™€ ê°™ì´ ê²½í—˜ìƒ 0.1%ì´ìƒ ì°¨ì´ê°€ ë‚˜ì§€ ì•Šì•„ì„œ. ë³¸ ë…¼ë¬¸ì—ì„œ ì„¤ëª…ëœ CNNì€ top-5 error rateìœ¼ë¡œ 18.2%ë¥¼ ë‹¬ì„±í•œë‹¤. 5ê°œì˜ ìœ ì‚¬í•œ CNNì˜ ì˜ˆì¸¡ì„ í‰ê· í•˜ë©´ 16.4%ì˜ ì˜¤ë¥˜ìœ¨ì´ ì œê³µëœë‹¤. ì „ì²´ 1,500ë§Œ ì´ë¯¸ì§€, 22K ì¹´í…Œê³ ë¦¬ë¥¼ ê°€ì§„ ImageNet Fall 2011 ë¦´ë¦¬ì¦ˆì…‹ì„ ë¶„ë¥˜í•˜ê¸° ìœ„í•´ ë§ˆì§€ë§‰ í’€ë§ ê³„ì¸µì—ë‹¤ê°€ 6ë²ˆì§¸ ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´ë¥¼ ì¶”ê°€í•œ CNN ëª¨ë¸ì„ í•™ìŠµì‹œí‚¨ë‹¤ìŒ ILSVRC-2012ì—ì„œ fine-tuningí•˜ë©´ error rateì´ 16.6%ê°€ ëœë‹¤. ì „ì²´ 2011ë…„ ê°€ì„ ë¦´ë¦¬ì¦ˆ ì…‹ì—ì„œ ì•žì„œ ì–¸ê¸‰í•œ 5ê°œì˜ CNNìœ¼ë¡œ pre-trained ëœ 2ê°œì˜ CNNì˜ ì˜ˆì¸¡ì„ í‰ê· í•˜ë©´ 15.3%ì˜ ì˜¤ë¥˜ìœ¨ì´ ì œê³µëœë‹¤.

AlexNetì„ ì½”ë“œë¡œ êµ¬í˜„í•œê²ƒì„ ì •ë¦¬í•œ íŽ˜ì´ì§€ì´ë‹¤. => [AlexNet](https://chaelin0722.github.io/deeplearning/cnn/code/AlexNet_code/)


#### ì°¸ê³ 

[1] [https://bskyvision.com/421](https://bskyvision.com/421)
