---
title:  "[논문정리] Gradient-Based Learning Applied to Document Recognition"
excerpt: "Week1 논문요약정리"

categories:
  - CNN
  - paperReview
tags: [CNN, PaperReview]

last_modified_at: 2021-05-28T08:06:00-05:00
classes: wide
---

Abstract :

  역전파 알고리즘으로 훈련된 다층신경망은 Gradient 기반 기술에 있어 최고의 성공사례이다. 특히 고차원 패턴을 가진 손글씨 같은 데이터를 적은 전처리만으로 좋은 성능으로 분류한다. 이 논문에서는 손글씨를 인식함에 있어 기존의 인식방법과 비교한 결과를 리뷰한다. CNN은 특히 2차원 형태의 데이터를 다루는데 다른 기법들보다 월등한 성능을 보여준다.

  실제 문서인식 시스템에서는 여러 모듈들로 구성되어 있는데 새로운 학습 패러다임인 GTN에 대해서 얘기한다. 전체적인 성능측정을 최소화하기위해 Gradient 기반학습을 한다. 온라인 손글씨 인식과 은행 수표를 읽는 것에 대한 두가지 시스템, 특히 은행 수표인식의 경우 CNN을 사용하며 비즈니스/개인 수표를 읽는데 기록 정확도를 높이며 실제 상업적으로 적용이 되어 하루에 몇 백만의 수표를 인식하고 있다.

## 1. introduction

  최근 몇 년 동안 머신러닝 기법이 특히 neural networks에 적용되는 것은 패턴인식 시스템에서 중요한 역할로 활용되었다. 실제로 학습기법의 유효성은 음성인식과 손글씨 인식과 같은 패턴인식 어플리케이션 성공에 중요한 요인이다.

  이 논문의 주 메시지 **”패턴인식 시스템은 hand-designed heuristics를 줄이고 자동화 학습에 주력해서 만드는 것이 더 좋은 성능을 가진다”**

  보통 패턴인식 시스템은 자동 학습 테크닉과 hand-craft 알고리즘의 조합으로 만들어진다. 개별 패턴을 인식하는 방법으로는 두 메인 모듈이 있다.

  (1) feature extractor: 입력패턴을 받아 낮은 차원의 특징 벡터로 변환하는 역할을 한다. 낮은 차원으로의 변환 이유는 (1-a) 후에 있을 분류 모듈에서 비교를 쉽게 하기위해, (1-b) 이 특징 벡터는    입력패턴의 변화나 왜곡에도 변하지 않는 특징을 갖도록 설계된다. 이런 이유로 사람에 의해 인위적으로 만들어진(hand crafted) 특징 추출알고리즘은 최대한 사전 지식을 활용하고 그 목적에 맞게 만    들어지곤 한다.

  (2) classifier: general purpose and trainable! 이 방법의 인식 정확도는 디자이너가 적절한 feature set을 만드는 능력에 크게 좌우된다. 문제점으로는 해결해야 할 문제를 새로 하면 feature도     다시 설계해야 하는 것이다.

  그 동안 적절한 특징추출의 필요성은 분류기에 사용된 학습기법이 나누어지기 쉬운 클래스의 저차원 공간에서는 제한되기 때문이었다. 하지만 세가지 요소의 combination이 이 인식을 바꾸게 했다.

  (1) 저렴해진 컴퓨터 가격과 성능 또한 올라서 brutal-force적인 풀이법이 가능

  (2) 거대 데이터 베이스가 생겨 손 글씨 같은 실제에 가까운 데이터 셋을 얻을 수 있음

  (3) 높은 차원을 다룰 수 있는 강력한 기계 학습 알고리즘이 나와 복잡한 결정을 내릴 수 있음

  이러한 변화로 기존의 인위적인 알고리즘을 통해 특징 벡터를 생성하는 방법 대신 픽셀 이미지를 직접 이용하는 학습 알고리즘을 이야기한다.

A. **learning from Data**

  Neural network에서 가장 유명한 접근방법으로 gradient-based learning이 있다.

  ![image](https://user-images.githubusercontent.com/53431568/119875430-b6878600-bf61-11eb-9b60-b0a512f9886b.png)
 => 학습기계는 이 식을 계산하는 것

  ![image](https://user-images.githubusercontent.com/53431568/119875440-b8514980-bf61-11eb-9ffa-795950171981.png)
 : p번째 입력패턴 , W: 파라미터, 가중치, : p번째 출력 값

  
![image](https://user-images.githubusercontent.com/53431568/119875450-ba1b0d00-bf61-11eb-8bf3-138250224a11.png) => loss function, 에러함수 E

  
![image](https://user-images.githubusercontent.com/53431568/119875458-bbe4d080-bf61-11eb-8038-d168fd9fd057.png) : 실제 정답 값

  즉 이 loss function은 입력 값이 가져야하는 출력 값(정답)
![image](https://user-images.githubusercontent.com/53431568/119875458-bbe4d080-bf61-11eb-8038-d168fd9fd057.png) 과 실제 함수로부터 나온 값
![image](https://user-images.githubusercontent.com/53431568/119875473-c010ee00-bf61-11eb-8459-3e27c5931ddd.png) 사이의 오차를 계산한다.

  ⇒ **에러 함수를 최소로 하는 파라미터(weight)를 찾는 과정**

B. **Gradient-Based Learning**

  Loss function의 값을 최소화하기 위해 기울기 ∇θ 를 이용하는 방법
  
  ![image](https://user-images.githubusercontent.com/53431568/119875496-c69f6580-bf61-11eb-9e7b-e738ee494740.png)

  **stochastic gradient algorithm** (on-line update / SGD)

  Regular gradient 보다는 빠르게 수렴한다.
  
  ![image](https://user-images.githubusercontent.com/53431568/119875508-c901bf80-bf61-11eb-926c-da3b98a9cf7a.png)

C.  **Gradient Back-Propagation**

  (1) Boltzmann machine과 같은 비선형 방법을 이용하면 local minima는 큰 문제가 아님

  (2) 여러 층으로 구성된 효과적인 Back-propagation방법 제안

  (3) 역전파 방법이 적용된 다층 신경망이 복잡한 문제를 풀 수 있음

  위의 세 사실이 밝혀진 후로 널리 쓰이기 시작했다.

D. **Learning in Real Handwriting Recognition Systems**

  글자로 분리된 손 글씨를 인식하는 문제는 오래전부터 연구되어왔으며 신경망으로 잘 풀었던 문제이다. 논문에서는 분리된 글자가 아닌 문자열에서 문자를 분리해 인식하는 문제 또한 어떻게 해결할지 제  안한다.

E. **Globally Trainable System**

이전까지의 패턴인식 시스템은 여러 모듈로 구성 되어있었다. 문서 인식시스템을 예로 들자면 입력으로 들어온 그림 중 글자가 들어있을 것 같은 관심영역을 추출하는 모듈, 추출한 관심영역을 다시 각 글자로 나누는 모듈, 각 글자가 무슨 글자인지 인식하는 모듈, 인식된 글자를 토대로 문맥을 파악하는 모듈, 그 후 문법을 검사하는 모듈 등의 형식으로 말이다. 따라서 각 모듈은 별도로 최적화 되어있고 훈련되어 있으며 전체 맥락에서 벗어나 있을 수밖에 없다.

더 나은 방법을 강구 -> **“전체 시스템을 한 번에 훈련시켜 최적화 시키는 방법**”

이 때에는 문서 차원에서의 잘못된 인식을 에러 함수로 사용해 모든 모듈에 해당하는 최적 파라미터들을 찾아야 한다. 이 에러함수 E가 시스템 파라미터 W에 대해 미분 가능하다면 이 또한 Gradient based learning으로 해결이 가능하다고 볼 수 있다.

에러 함수를 미분 가능하도록 만들기 위해 전체 시스템을 미분 가능한 feed-forward network를 이용해 구성한다. 이때 각 모듈에서 사용되는 함수들은 입력으로 들어올 수 있는 값과 파라미터에 대해 연속이어야 하고 미분가능해야 한다. 또한 에러함수의 기울기를 계산하기 위해 역전파 알고리즘을 적용하면 다음과 같은 형태이다


![image](https://user-images.githubusercontent.com/53431568/119875862-37df1880-bf62-11eb-9181-49a8337cbfa4.png)

첫번째 식은 에러함수의 기울기, 두번째는 역전파의 backward recurrence값으로 사용된다

## 2. Convolutional Neural Networks for Isolated character recognition

  패턴인식의 전통모델인 hand-designed 특징추출은 입력으로부터 관련있는 정보를 모으고 관련없는 요소들은 제거한다. Trainable classifier은 결과 특징벡터를 클래스들로 묶는다. 기존의 fully-connected 다중 레이어 네트워크는 분류기로 사용했다. 특징 추출 자체로 학습하는 것 특히 문자 인식에서 네트워크는 거의 날것의 입력을 제공한다. 이러한 것들은 fully connected-feed-forward 네트워크로 되는데 몇 가지 문제가 있다.

  문제1. 2D 이미지 정보가 실제로는 많은 pixel로 구성 되어 있어 이것을  fully-connected로만 학습시키면 너무 많은 학습량, 학습시간, 많은 weight를 저장해야 할 hardware적인 요소 필요

  문제2. Fully-connected layer에 이미지를 넣으면 3차원의 데이터를 1차원으로 변경해야 한다. 이때 데이터의 형상이 무시된다. 아래 이미지처럼 1차원의 flat 데이터로 펼쳐줘야 했다.


![image](https://user-images.githubusercontent.com/53431568/119876065-707ef200-bf62-11eb-95db-04a15cd428a9.png)

  ==> 이러한 문제로 CNN을 사용하여야 한다.

 A. Convolutional Networks

  CNN의 각 층들은 local receptive field, shared weight, sub-sampling 이 세가지로 구성되어 있다.

  **local receptive field:** CNN의 한 유닛은 완전 연결 구조처럼 이전 층의 모든 영역에서 입력을 받는 구조가 아닌 이전 층에서의 지역적으로 이웃하는 일부 유닛들에게서만 입력을 받는다. 이 방법으로 이미지의 엣지나 선의 모서리, 코너 등 2차원적인 특징을 잘 반영한다. 전체 이미지에서 왜곡이나 움직임이 발생해도 결국 각 클래스를 구분짓는 패턴 특징이 local receptive field에서 만나기 때문에 해당 특징을 반영한 feature map을 만들어낼 수 있다.

  **shared weight:** local receptive field에 따라 CNN은 2차원적인 구조를 가지게 되는데 이렇게 생성된 2차원 구조의 유닛들의 집합을 feature map(특징맵)이라고 하자. Feature map에서 다음 층의 feature map으로 입력이 이루어질 때 사용되는 가중치들은 입력 유닛과 대상 유닛의 위치만 달라질 뿐 local receptive fields내에서의 가중치들은 같은 값을 사용하도록 한다. 따라서 한 feature map에서 각 유닛의 계산은 위치만 다를 뿐 같은 계산식을 갖게 되는 것이고 이는 이미지 처리에서의 convolution과 정확히 같은 계산이다. 이러한 계산의 특징으로서 입력 글자의 위치가 변하더라도 feature map의 계산 결과 또한 그 값이 변하지 않고 다른 위치에 그대로 나타나게 된다. 이러한 기법으로 계산을 수행할 local machine에게 요구되어지는 총 계산 capacity를 줄여주고, 학습할 parameter의 수를 줄여 자연스럽게 Overfitting을 방지하게 되어 test error와 training error 사이의 gap도 줄여준다.

  **sub-sampling:** pooling의 개념과 같다. Lenet-5에서는 Average Pooling을 사용한다. local feature로부터 입력된 데이터의 translation, distortion에 관계없이 위상에 영향을 받지 않는 global feature를 추출하기 위해 사용한다.  위치 변화에 따라 특징 값은 변하지 않도록 고려하였으나 그 위치 정보 또한 여전히 민감하게 작용하는 부분이다. 글자에서 코너나 선 끝 부분은 어떤 글자인지 판단하는데 아주 중요한 특징이다. 또, 이미지의 노이즈나 왜곡도 고려되어야 한다. 따라서 feature map의 해상도를 줄이는 방법을 사용한다.

  CNN에서 사용되는 층은 그 feature map을 생성하는 방법에 따라 Convolutional layer, Subsampling layer로 크게 2가지로 나눌 수 있다.

  **Convolutional layer**는 local receptive fields와 shared weight의 조합으로 Convolution 계산을 하는 층이다. 이 층은 이전 층을 토대로 feature map을 생성하게 되는데, 하나가 아닌 여러 개의 feature map을 생성하여 구성된다. 또한 입력으로 받는 이전 층의 유닛에 대해서도 하나 이상의 feature map에게 연결된다. 다시 이야기하자면, 한 층은 여러 개의 feature map으로 구성되며 각 feature map은 이전 층의 1개 이상의 feature map으로부터 영향을 받고 자신 또한 다음 층의 여러 feature map에 영향을 준다. Shared weight에 의하여, 각 feature map 내에서는 같은 가중치를 사용하여 Convolution을 한 다음 bias 값을 더하여 활성함수를 통과시켜 출력한다.

  **Subsampling layer**는 Convolutional layer와 달리 이전 층의 한 feature map에게서만 영향을 받는다. 따라서 이전 층과 같은 수의 feature map을 갖게 된다. 이전 층의 일정 영역을 평균한 가중치를 곱하고 bias 값을 더한다. 이 때, Convolutional layer와는 다르게 계산에 사용하는 영역은 서로 겹치지 않도록 배치하므로 영역의 크기에 반비례하게 feature map의 크기가 작아지게 된다.

  이렇게 구성된 각 층의 feature map은 back propagation을 통하여 서로 다른 가중치를 갖도록 학습되므로, 이들은 각자가 서로 다른 특징을 추출하게끔 하는 특징 추출기라고 볼 수 있다.

 B. **LeNet-5**

  LeNET-5는 글자 인식을 위한 신경망으로 총 7개의 층으로 이루어져 있으며, 각 층은 Convolutional layer와 Subsampling layer으로 구성되어 있다.


![image](https://user-images.githubusercontent.com/53431568/119876106-7c6ab400-bf62-11eb-87cb-9568e9362f54.png)

  먼저 입력 글자 이미지는 크기를 32*32로 모두 같도록 하고, 글자가 가운데 위치하도록 정렬된 상태로 미리 처리해 놓는다. 각 픽셀의 값은, 흰색(배경)은 -1.0으로, 검은색(글자)은 1.175로 정규화 한다.

  1) 이렇게 만든 입력은 첫번째 C1 층과 연결된다. C1 층은 convolutional layer이며, 6개의 feature map으로 구성된다. 입력 이미지의 5*5 영역이 각 feature map의 한 유닛으로 연결되도록 한다. Convolution 계산 시 경계를 고려하여 각 feature map은 (상하좌우 경계에 32 - 5*6 = 2를 빼준다, 즉 32-4 * 32-4 ) 28*28 크기를 갖는다.
  
  #### > Trainable parameter: (가중치*입력맵개수 + 바이어스)*특성맵개수 = (5 * 5 * 1 + 1 ) * 6 = 156
 (가중치가 곧 mask 이므로 5*5가 가중치가 된다.)

 ⇒ 아마 글자는 가운데 정렬이므로 경계는 빼줘도 상관 없나보다.. 내 생각임

  2) 두번째 층은 S2로 subsampling layer이다. C1 층의 각 feature map 상 2*2 영역을 입력으로 하여 Subsampling 과정을 거치면 14*14로 크기가 줄어든 6개의 feature map을 생성된다. (= 기억하자 subsampling은 반비례하게 작아지게 된다는 것!)

  #### > Trainable parameter: (가중치 + 바이어스)*특성맵개수 = (1 + 1)*6 = 12

  ⇒ 가중치가 1인 이유는 subsampling은 average pooling기법을 사용한다. Average pooling은 평균을 낸 후 한 개의 훈련가능한 가중치를 곱해주고 또 한 개의 훈련가능한 바이어스를 더해주기 때문. 이 값이 시그모이드 함수를 통해 활성화되고 이 가중치와 바이어스는 시그모이드의 비활성도를 조절해준다.

  3) 다음 C3 층은 다시 convolutional layer을 사용하나 앞의 C1과는 다르게 S2의 여러 feature map들을 한꺼번에 참조하여 C3의 한 유닛으로 연결된다. 다시 말하면 마치 3차원 convolution을 하는 것과 같은 계산이다. 참조하는 feature map은 미리 정해 놓은 규칙에 따라 S2층의 3~5 개 feature map들에서 5*5 영역을 이용한다. 마찬가지로 경계를 고려하여 C3 층의 크기는 10*10으로 줄어들게 된다.

![image](https://user-images.githubusercontent.com/53431568/119876148-87bddf80-bf62-11eb-8630-c1fe03015a40.png)

  **> Trainable parameter: 456 + 606 + 303 + 151 = 1516**

  (가중치*입력맵 개수 + 바이어스)*특성맵 개수

  첫번째 => (5*5*3+1)*6 = 456 (연속한 3장씩 5*5*3 필터와 컨볼루션)

  두번째 => (5*5*4+1)*6 = 606 (연속한 4 장씩 5*5*4 필터와 컨볼루션)

  세번째 => (5*5*4+1)*3 = 303 (불연속한 4장씩 5*5*4 사이즈의 필터와 컨볼루션)

  네번째- => (5*5*6+1)*1 = 151 (6장의 14*14 특성맵 모두를 가지고 필터와 컨볼루션)

  따라서 16장의 10*10 특성맵이 산출된다.

  4) 이어서 S2 층과 같은 방법으로 S4층을 생성한다. 5*5 크기의 16개 feature map을 갖는다.

  **> Trainable parameter = (1 + 1)*16 = 32**

  5) 다음 C5 층은 convolutional layer로 120개의 feature map으로 구성되는데, 5**5 영역을 사용하므로 이전 층의 크기를 생각하면 각 feature map은 1*1*크기를 갖게 된다. 각 유닛들은 S4층    의 모든 feature map에서 입력 받도록 구성하여 fully connected graph 형태를 갖도록 한다.

  **> Trainable parameter = (5*5*16 +1)*120 = 48120**

6) F6 층은 C5 층과 완전히 연결된(fully connected) 84개의 유닛으로, 마지막으로 10개의 유닛들이 F6 층과 완전히 연결된 출력 층을 만들어 신경망 구성을 마무리한다.

  **> Trainable parameter = (120 + 1)*84 = 10164**

  F6 까지는 기존의 신경망과 같이 가중치 합에 bias를 더한 뒤 이를 활성함수에 통과시켜 출력 값으로 사용한다. 이 때, 사용된 함수는 아래와 같다.
  
![image](https://user-images.githubusercontent.com/53431568/119876188-960bfb80-bf62-11eb-8b9e-9203e4e2b8ee.png)


  이 때, A는 1.7159, a는 가중치 합에 bias를 더한 입력 값이고, S는 원점에서의 기울기를 결정하는 파라미터이다.

7) 마지막 출력 층은 Radial Basis Function (RBF)을 적용하여 한 유닛들로 구성되며 최종적으로 이미지가 속한 클래스를 알려주며,


![image](https://user-images.githubusercontent.com/53431568/119876272-acb25280-bf62-11eb-81fa-648426628593.png)

  위와 같이 계산된다. 이 때 사용되는 가중치 벡터 w는 논문에 나와있는 7*12 크기의 ASCII 비트맵 이미지를 참고하여 -1, +1 로 구성한다.

  ( RBF는 방사형 구조를 기본으로 하는 네트워크로서 1개의 은닉층에 확률 가우시안이 적용된다. )

  LeNet-5를 제대로 가동하기 위해 훈련해야 할 파라미터는

  **총 156 + 12 + 1516 + 32 + 48120 + 10164 = 60000개**

C.  **Loss Function**

  ****가장 단순한 함수로 아래와 같이 사용 가능하다.

![image](https://user-images.githubusercontent.com/53431568/119876315-b8057e00-bf62-11eb-88c2-9be001859fc7.png)

  는 한 입력 샘플 p가 속하는 클래스인 의 RBF 유닛이다. 앞의 RBF유닛의 식을 살펴보면 가장 이상적인 경우 이 값은 0이 되므로 E(W)가 작을수록 잘 훈련된 상태로 볼 수 있다. 여기에 변별력을 높이기 위해 0이 나오면 안되는 다른 RBF유닛들에 대한 페널티 항을 넣으면 다음과 같다.


![image](https://user-images.githubusercontent.com/53431568/119876347-c2277c80-bf62-11eb-8839-9cc2b09a6012.png)

  J는 상수로 LOG의 값이 너무 작아지는 것을 방지한다. 결과적으로 페널티는 
![image](https://user-images.githubusercontent.com/53431568/119876377-cce21180-bf62-11eb-8303-98f61daed16a.png)와  
![image](https://user-images.githubusercontent.com/53431568/119876408-d2d7f280-bf62-11eb-8e56-1c019fcaac1a.png) 사이의 비율로 결정된다.

  Loss function을 사용해 모든 층의 가중치들을 back-propagation으로 훈련시킨다. 각 feature map 마다 가중치를 공유해 사용하기 때문에 약간의 트릭을 이용해 계산을 효율적으로 할 수 있다.

## 3. **Results and Comparison with other methods**

  현존하는 많은 방법들이 hand-crafted 특징 추출과 학습가능한 분류기를 조합하지만 이 논문은 크기가 정규화된 이미지들을 직접 다루는 적용가능한 방법에 초첨을 맞춘다.

1. Database: the Modified NIST set

  MNIST 데이터베이스는 60,000개의 트레이닝 이미지와 10,000개의 테스트 이미지를 포함한다. 트레이닝 세트의 절반과 테스트 세트의 절반은 NIST의 트레이닝 데이터셋에서 취합하였으며, 그 밖의 트레이닝 세트의 절반과 테스트 세트의 절반은 NIST의 테스트 데이터셋으로부터 취합되었다 (위키피디아)

 2. Results

  여러 버전의 LeNet-5가 regular MNIST 데이터베이스로 훈련되었다.

![image](https://user-images.githubusercontent.com/53431568/119876467-e2573b80-bf62-11eb-9ce9-7a537f3419f5.png)
![image](https://user-images.githubusercontent.com/53431568/119876516-ee42fd80-bf62-11eb-8e52-9553c5230b55.png)

  훈련 셋이 클수록 LeNet-5의 성능을 향상시킬 수 있다.

---

## LeNet-5 구조


![LENET_1](https://user-images.githubusercontent.com/53431568/119873337-68718300-bf5f-11eb-9563-6745667e819e.JPG)
![LENET_2](https://user-images.githubusercontent.com/53431568/119873340-69a2b000-bf5f-11eb-8b0b-a294d57a206c.JPG)
![LENET_3](https://user-images.githubusercontent.com/53431568/119873343-6a3b4680-bf5f-11eb-9710-8e00592c3232.JPG)
![LENET_4](https://user-images.githubusercontent.com/53431568/119873344-6a3b4680-bf5f-11eb-859a-ea54d3f96bb1.JPG)

이 신경망을 코드로 구현한것을 정리한 페이지이다. => [LeNet-5](https://chaelin0722.github.io/deeplearning/cnn/code/LENET-5_code/) 



#### 참고

[1] [http://www.navisphere.net/1831/gradient-based-learning-applied-to-document-recognition/](http://www.navisphere.net/1831/gradient-based-learning-applied-to-document-recognition/)

[2] [https://bskyvision.com/418](https://bskyvision.com/418)

[3] [https://ctkim.tistory.com/119](https://ctkim.tistory.com/119)
