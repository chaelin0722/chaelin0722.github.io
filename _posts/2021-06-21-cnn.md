---
title:  "[딥러닝공부] 합성곱 신경망(Convolutional Neural Network)"
excerpt: "CNN"

categories:
  - Deeplearning
  - CNN
  - study
tags: [Deeplearning, CNN,study]
use_math: true
classes: wide

last_modified_at: 2021-06-21T08:06:00-05:00
---

## 합성곱 신경망(Convolutional Neural Network)

논문정리를 하다보니 컨볼루션 연산 개념이 헷갈려서 CNN 개념을 다잡고자 한다. 

[위키독스](https://wikidocs.net/64066)라는 곳에서 자연어 처리를 위한 딥러닝 개념들을 잘 기술한 것 같아 이 [사이트](https://wikidocs.net/64066)를 참고해 공부하였다. 

## 1. 합성곱 신경망(CNN)
합성곱 신경망은 이미지 처리에 탁월한 성능을 보이는 신경망이다. 

CNN에서 사용되는 층은 그 feature map을 생성하는 방법에 따라 합성곱층(Convolutional layer), 풀링층(Pooling layer 혹은 Subsampling layer)로 크게 2가지로 나눌 수 있다.

이 두 레이어에 대해 자세히 알고싶다면 '더보기🔎'를 참고!

<details markdown="1">
<summary>더보기🔎</summary>

#### - Convolutional layer
Convolution 계산을 하는 층이다. 이 층은 이전 층을 토대로 feature map을 생성하게 되는데, 하나가 아닌 여러 개의 feature map을 생성하여 구성된다. 
또한 입력으로 받는 이전 층의 유닛에 대해서도 하나 이상의 feature map에게 연결된다. 각 feature map 내에서는 같은 가중치를 사용하여 Convolution을 한 다음 bias값을 더하여 활성함수를 통과시켜 출력한다.

#### - Subsampling layer
Pooling Layer과 같은 개념으로 Convolutional layer와 달리 이전 층의 한 feature map에게서만 영향을 받는다. 따라서 이전 층과 같은 수의 feature map을 갖게 된다. 
이전 층의 일정 영역을 평균한 가중치를 곱하고 bias 값을 더한다. 이 때, Convolutional layer와는 다르게 계산에 사용하는 영역은 서로 겹치지 않도록 배치하므로 영역의 크기에 반비례하게 feature map의 크기가 작아지게 된다.

</details>

## 2. 합성곱 신경망 출현이유

이미지 처리를 위해 다층 퍼셉트론 (MultiLayer Perceptron)을 사용하는데에 한계가 있다. 예를 들면 아래와 같이 알파벳 손글씨를 분류하는 문제가 있다고 가정해보자.

아래의 그림은 알파벳 Y를 서로 다른 글씨체로 쓴 것을 2차원 텐서인 행렬로 표현한 것이다.

![FIRST](https://user-images.githubusercontent.com/53431568/122728393-0f251700-d2b3-11eb-9099-d6e274df4012.PNG)

사람은 알파벳 Y로 판단할 수 있지만 기계가 볼때는 각 픽셀마다 가진 값이 상이하기 때문에 완전 다른 값을 가진 입력으로 인식한다. 

위 손글씨를 다층 퍼셉트론으로 분류한다고 하면 이미지를 1차원 텐서인 벡터로 변환하고 다층 퍼셉트론의 입력층으로 사용해야 한다. 따라서 두번째 손글씨를 다층 퍼셉트론으로 분류하기 위해 벡터로 변환하면 다음과 같다.


![(2)](https://user-images.githubusercontent.com/53431568/122728428-19dfac00-d2b3-11eb-8048-0d936b5f0e44.jpg)

1차원으로 변환된 결과는 공간적 구조정보가 유실되어있는 상태이다. 공간적 구조(spatial structure)란 거리가 가까운 어떤 픽셀들끼리는 어떤 연관이 있고 어떤 픽셀들끼리는 값이 비슷하거나 등의 정보를 포함하고 있다.
결국 이미지의 `공간적 구조 정보를 보존하면서 학습할 수 있는 방법의 필요성으로 인해 합성곱 신경망`을 사용하게 되었다.


## 3. 채널(Channel)
기계는 글자나 이미지보다 숫자(텐서)를 더 잘 처리할 수 있다. 

이미지는 (높이, 너비, 채널)이라는 3차원 텐서이다. 여기서 높이는 이미지의 세로 방향 픽셀 수, 너비는 이미지의 가로 방향 픽셀 수, 채널은 색 성분을 의미합니다. 흑백 이미지는 채널 수가 1이며, 각 픽셀은 0부터 255 사이의 값을 가집니다. 아래는 28 × 28 픽셀의 손글씨 데이터를 보여준다.

![8](https://user-images.githubusercontent.com/53431568/122728418-164c2500-d2b3-11eb-8514-e4fa470f6a33.PNG)

위 손글씨 데이터는 흑백 이미지므로 채널 수가 1임을 고려하면 (28 × 28 × 1)의 크기를 가지는 3차원 텐서이다. 

한편, 흑백이 아닌 컬러 이미지는 red, green, blue 로 3개의 채널로 이루어진 이미지이다. 따라서 하나의 픽셀은 세 가지 색의 조합으로 이루어지게 된다. 

만약, 높이가 28, 너비가 28인 컬러 이미지가 있다면 이 이미지의 텐서는 (28 × 28 × 3)의 크기를 가지는 3차원 텐서가 된다. 채널은 깊이(depth)라고도 하는데 이 경우 이미지는 (높이, 너비, 깊이)라는 3차원 텐서로 표현된다고 말할 수 있을 것이다.

## 4. 합성곱 연산 (Convolution Operation)

합성곱층은 `합성곱 연산을 통해서 이미지의 특징을 추출`하는 역할을 한다. 합성곱은 영어로 컨볼루션이라고도 불리는데, 커널(kernel) 또는 필터(filter)라는 크기의 행렬로 높이너비 크기의 이미지를 처음부터 끝까지 겹치며 훑으면서 크기의 겹쳐지는 부분의 각 이미지와 커널의 원소의 값을 곱해서 모두 더한 값을 출력으로 한다. 이때, 이미지의 가장 왼쪽 위부터 가장 오른쪽까지 순차적으로 훑으면서 계산한다. 

- 커널의 크기는 사용자가 지정할 수 있으며 일반적으로 3 × 3 또는 5 × 5와 같이 홀수X홀수로 사용한다.

- 커널의 이동 범위를 stride라고 한다.

아래는 3X3 크기의 커널로 5X5의 이미지 행렬에 stride = 1인 합성곱 연산을 수행하는 과정을 나타내었다. 

#### 1. 첫 번째 연산
![STEP1_](https://user-images.githubusercontent.com/53431568/122728385-0df3ea00-d2b3-11eb-95e9-9f761076ebd6.PNG)

(1X1) + (2X0) + (3X1) + (2X1) + (1X0) + (0X1) + (0X0) + (2X1) + (4X0) = 8


#### 2. 두 번째 연산
![STEP2_](https://user-images.githubusercontent.com/53431568/122728391-0e8c8080-d2b3-11eb-8f78-3e300218d040.PNG)

(2X1) + (3X0) + (4X1) + (1X1) + (0X0) + (1X1) + (2X0) + (4X1) + (1X0) = 12

위의 연산을 총 9번 모두 하게 되면 최종 결과는 다음과 같다. 


![캡처](https://user-images.githubusercontent.com/53431568/122733891-93c66400-d2b8-11eb-9060-3e16ece38053.PNG)

이렇게 입력으로 부터 커널을 사용해 합성곱 연산을 통해 나온 결과를 **특성 맵(feature)**이라고 한다.



## 5. 패딩(padding)
위의 예시를 통해 5 × 5 이미지에 3 × 3의 커널로 합성곱 연산을 하였을 때, 스트라이드가 1일 경우에는 3 × 3의 특성 맵을 얻은것을 확인하였다!

이와 같이 합성곱 연산의 결과로 얻은 `특성 맵은 입력보다 크기가 작아진다`는 특징이 있습니다. 만약, 합성곱 층을 여러개 쌓았다면 최종적으로 얻은 특성 맵은 초기 입력보다 매우 작아진 상태가 되버린다. 합성곱 연산 이후에도 특성 맵의 크기가 입력의 크기와 동일하게 유지되도록 하고 싶다면 **패딩(padding)** 을 사용하면 된다.

![padding](https://user-images.githubusercontent.com/53431568/122734554-26ff9980-d2b9-11eb-986d-e521b6c20f6a.PNG)

패딩은 (합성곱 연산을 하기 전에) 입력의 가장자리에 지정된 개수의 폭만큼 행과 열을 추가해주는 것을 말한다. 

주로 값을 0으로 채우는 제로 패딩(zero padding)을 일반적으로 사용한다. 위의 그림은 5 × 5 이미지에 1폭짜리 제로 패딩을 사용하여 각각  상,하,좌,우에 1칸씩 추가한 모습을 볼 수 있다.

> 💡 만약 스트라이드가 1이라고 하였을 때, 3 × 3 크기의 커널을 사용한다면 1폭짜리 제로 패딩을 사용하고, 5 × 5 크기의 커널을 사용한다면 2폭 짜리 제로 패딩을 사용하면 입력과 특성 맵의 크기를 보존할 수 있습니다. 예를 들어 5 × 5 크기의 이미지에 1폭짜리 제로 패딩을 하면 7 × 7 이미지가 되는데, 여기에 3 × 3의 커널을 사용하여 1 스트라이드로 합성곱을 한 후의 특성 맵은 기존의 입력 이미지의 크기와 같은 5 × 5가 됩니다.


<br>
(아직 수정중인 페이지)
## 6. 가중치(weight)와 편향(bias)

### - 합성곱 신경망의 가중치(weight)
  <U>다층 퍼셉트론으로 3x3 이미지를 처리</U>한다고 가정해보자. 우선 이미지를 1차원 텐서인 벡터로 만들면 3 x 3 = 9 가 되므로 입력층은 9개의 뉴론을 가지게 된다. 그 다음으로 4개의 뉴런을 가지는 은닉층을 추가한다고 해보자. 아래 이미지를 참고해보자!

차례대로 **입력 이미지  /           입력층  /       은닉층**
![0622](https://user-images.githubusercontent.com/53431568/122792299-01908100-d2f5-11eb-8182-d25d828d38ca.jpg)

다음과 같이 각 연결선은 가중치를 의미한다고 할 때, 9 x 4 = 36개의 가중치를 가진다.

이번엔 비교를 위해 <U>합성곱 신경망으로 3x3 이미지를 처리</U>한다고 해보자. 커널사이즈는 2X2, stride는 1로 가정한다. (여기서 * 는 합성곱 연산을 의미한다.)

합성곱 신경망에서 가중치는 커널 행렬의 원소들이다. 오른쪽 이미지는 인공 신경망의 형태로 표현한 그림이다.

![CONV](https://user-images.githubusercontent.com/53431568/122793795-8f20a080-d2f6-11eb-83fa-6f03d1363e7e.jpg)


최종적으로 특성 맵을 얻기 위해 동일한 커널로 이미지 전체를 훑으며 합성곱 연산을 진행한다. 결국 이미지 전체를 돌아다니며 사용되는 가중치는 $W_0$, $W_1$, $W_2$, $W_3$으로 4개 뿐이다. 그리고 각 합성곱 연산마다 이미지의 모든 픽셀을 사용하는 것이 아니라, 커널과 맵핑되는 픽셀만을 입력으로 사용하는다. 

> ⭐️ 합성곱 신경망은 다층 퍼셉트론을 사용할때보다 훨씬 적은 수의 가중치를 사용해 공간적 구조정보를 보존한다는 특징이 있다. 

다층퍼셉트론의 은닉층에는 가중치 연산 후 비선형성(non-linearlity)를 추가하기 위해 활성화 함수를 통과시켰다. 합성곱 신경망의 은닉층에서도 마찬가지로 합성곱 연산을 통해 얻은 특성 맵은 비선형성 추가를 위해 활성화 함수를 적용한다. 이때 ReLU함수나 ReLU함수의 변형들이 주로 사용된다. 
ReLU함수에 대해 자세히 알고싶다면 저번 포스팅에서 다룬 [활성화 함수 포스팅](https://chaelin0722.github.io/deeplearning/cnn/study/neural-network/) 참고! => [ReLU함수](https://chaelin0722.github.io/deeplearning/cnn/study/neural-network/)

=> 이와 같이 합성곱 연산을 통해 특성맵을 얻고 활성화 함수(activation function)를 지나는 연산을 하는 합성곱 신경망의 은닉층을 합성곱 신경망에서는 합성곱 층이라고 한다.

### - 합성곱 신경망의 편향(bias)
합성곱 신경망에도 편향을 추가할 수 있다. bias는 커널 적용 후에 더해진다. bias는 하나의 값만 존재하며 커널이 적용된 결과의 모든 원소에 더해진다. 따라서 만약 컨볼루션된 결과가 차례로 3, 3, 5, 1 이고 bias가 5 이면 특성맵의 결과는 8, 8, 10, 6 이 된다.

## 7. 특성맵 크기 계산 방법
입력의 크기와 커널의 크기, stride 값만 알면 합성곱 연산의 결과인 특성 맵의 크기를 계산할 수 있다.

- $I_h$ : 입력높이
- $I_w$ : 입력너비
- $K_h$ : 커널높이
- $K_w$ : 커널너비
- $S$ : stride
- $O_h$ : 특성맵 높이
- $O_w$ : 특성맵 너비

특성 맵의 높이와 너비는 다음과 같이 표현할 수 있다.

$O_h = floor(\frac{I_h-K_h}S + 1)$

$O_w = floor(\frac{I_w-K_w}S + 1)$

여기서 floor함수는 소수점 발생 시 소수점 이하를 버리는 역할을 한다.

또한 패딩의 폭을 P라고 하고 패딩까지 고려한 식은 다음과 같다.

$O_h = floor(\frac{I_h-K_h+2P}S + 1)$

$O_w = floor(\frac{I_w-K_w+2P}S + 1)$

## 8. 다수의 채널을 가질 경우의 합성곱 연산(3차원 텐서의 합성곱 연산)
실제 합성곱의 연산은 다수의 채널을 가진 이미지 혹은 이전 연산의 결과로 나온 특성맵일 것이다. 이런 계산을 위해 데이터의 채널 수와 커널의 태널 수는 같아야 한다. 채널수는 같으므로 합성곱 연산을 채널마다 수행하게 되며 그 결과를 모두 더해 최종 특성 맵을 얻게된다.

![image](https://user-images.githubusercontent.com/53431568/122796485-69e16180-d2f9-11eb-8b4e-705022af5639.png)

## 9. 3차원 텐서의 합성곱 연산
3차원 텐서의 각 차원을 변수로 두고 일반화 시키면 각 변수가 의미하는 바는 아래와 같다.
- $I_h$ : 입력높이
- $I_w$ : 입력너비
- $K_h$ : 커널높이
- $K_w$ : 커널너비
- $O_h$ : 특성맵 높이
- $O_w$ : 특성맵 너비
- $C_i$ : 입력 데이터의 채널


![image](https://user-images.githubusercontent.com/53431568/122798315-64851680-d2fb-11eb-90a7-d62cb6db2a69.png)
높이 $I_h$, 너비 $I_w$, 채널 $C_i$의 입력 데이터는 동일한 채널수 $C_i$를 가지는 높이 $K_h$, 너비 $K_w$의 커널과 합성곱 연산으로 높이 $O_h$, 너비 $O_w$, 채널 = 1의 특성맵을 얻는다.

그렇다면 여러개의 커널을 사용하는 연산은 어떻게 되는가? 🤔

=> 합성곱 연산에서 다수의 커널을 사용한다면 연산과정은 다음과 같다.

![image](https://user-images.githubusercontent.com/53431568/122798834-00168700-d2fc-11eb-92cf-41ad517f74ce.png)

기증치는 커널 원소들이므로 하나의 커널의 하나의 채널은 $K_i$X$K_o$개의 매개변수를 가지고 있다. 그런데 합성곱 연산을 하려면 커널은 입력 데이터의 채널 수와 같아야 한다. 따라서 하나의 커널이 가지는 매개변수의 수는 $K_i$X$K_o$X$C_i$가 된다. 또, 이러한 커널이 총 $C_o$개가 있어야 하므로 가중치 매개변수의 총 수는 다음과 같다.

#### 총 가중치 매개변수 : $K_i$X$K_o$X$C_i$X$$

## 10. 풀링(Pooling)
합성곱 층(합성곱 연산 + 활성화 함수) 다음에는 풀링 층을 추가하는 것이 일반적이다.

[[논문정리]](https://chaelin0722.github.io/categories/paperReview) 포스팅들을 참고해보면 대부분의 네트워크 구조가 

`CONVOLUTIONAL LAYER -> ACTIVATION FUNCTION -> SUBSAMPLING(POOLING)` 순서인 것을 알 수 있다.

풀링층에서는 특성맵을 다운샘플링해 특성 맵의 크기를 줄인다(반비례하게됨) 일반적으로 max pooling과 average pooling이 사용된다.

![image](https://user-images.githubusercontent.com/53431568/122797903-f04a7300-d2fa-11eb-9c4d-041ea05caec6.png)

> 풀링을 사용하면, 특성 맵의 크기가 줄어드므로 특성 맵의 가중치의 개수를 줄여줍니다




