---
title:  "[논문정리📃] Frame Attention Networks for facial expression Recognition in videos"
excerpt: "-FAN-"

categories:
  - fer
tags: [fer, CNN, paperReview]
use_math: true

last_modified_at: 2021-11-02T08:06:00-05:00
classes: wide
---

## Frame Attention Networks for facial expression Recognition in videos
#### - FAN - 

[논문원본](https://arxiv.org/pdf/1907.00193.pdf)😙


이 논문은 비디오(영상)을 프레임처리하여 얼굴표정을 인식하는데의 한계를 지적하고 `relation-attention` 이라는 개념을 추가한 CNN 모델을 제안하고 있습니다.

<br>

먼저, Frame을 처리하여 FER(Facial Express Recognition)을 하는데 self-attention이라는 개념을 도입한 배경을 알아보겠습니다.

일단, 영상을 처리하기 위해서는 한 비디오 파일을 frame(이미지 파일)으로 바꾸어서 처리를 해야하는데, 각각의 프레임들에 대한 얼굴의 특징을 찾는 것은 매우 중요한 일입니다. 

따라서 이 논문에서는 프레임워크의 종단 학습에 있어서 *차별적인 부분을 자동으로 하이라이트 할 수 있는 방법을 제안하고 있습니다.

그렇다면 여기서 차별적인 부분이란, 각 프레임들이 가지는 중요도를 말하고 있는데요, 자세한 것은 천천히 설명하겠습니다. 

일단 video-based FER에 있어서 특징을 추출하는 방법에는 3가지가 있습니다.

> 1) static-based feature extraction (특징점 기반 추출)
>  
>  ex) LBP, Gabor filters 
> 
> 2) spatial-temporal methods (시계열, 시공간을 인식하는 방법)
>
>  ex) LSTM, C3D
> 
> 3) Geometry based methods (얼굴의 key point를 추출하여 사용하는 방법)

위의 3가지 방법에서 1) 번의 방법이 EmotiW 라는 challenge에서 가장 좋은 성능을 보이는 방식이라 이 방식을 사용한다고 합니다.

그런데 문제는 여기서 1) 번의 방식을 사용하려면 Frame aggregation 이라는 것을 해야합니다. 이 논문 이전의 논문을 살펴보면은 고정된 길이의
video representation 값은 n개의 클래스의 확률분포 벡터로 형성하는데 이것을 프레임들의 평균 혹은 확장의 방식으로 묶어서 처리한다고 합니다.

아래 수식을 보면

$r=\sum_k a_kf_k$  에서, $a_k$는 linear weight, $f_k$는 feature representation에서 추출한 feature extraction(특징점) k는 비디오의 k 번째 프레임이며, 
$r$은 representation 값을 뜻합니다.

각 프레임에 대해서 linear weight, 여기서는 하이퍼 파라미터 인 것 같습니다. 논문에서 이 weight 값에 따라 성능의 차이가 있다고 말한 것을 미루어 보아 임의로 주어지는 랜덤값인 것 같습니다.

이런식으로 각 프레임에 대해서 가중치를 곱한 것들의 합으로 하나의 representation 값으로 본다는 것입니다.

하지만 아래 사진을 보면, 한 비디오에 대해 frame으로 쪼개지면서 각 프레임이 확실한 표정을 보이는 것도 있지만 애매하게 생각될 수 있는 프레임도 있는데, 
이것에 대한 고려 없이 (각 프레임의 표정의 확실한 정도와는 상관없이) 가중치를 랜덤하게 주어버리는 것입니다. 예를 들면 happy 와는 사뭇 달라보이는 
마지막 프레임에 들어간 랜덤 가중치($a_k$)가 커버린다면, 학습할 때 애매한 데이터의 비중이 더 커지는 불상사가 일어나겠죠!😵😵

![image](https://user-images.githubusercontent.com/53431568/139844228-03f656b0-98b2-4ade-a78f-45be59ab5fb7.png)


따라서, 이 논문에서는 위와 같은 방식으로는 각 frame 들에 대한 중요도를 무시하는 경향이 있다며 한계를 지적하였습니다.

그렇게 해서 각 프레임에 대해 중요한 정도를 판별해 `중요도의 가중치를 주자`! 하고 나온것이 FAN 입니다.


## Network Architecture

![image](https://user-images.githubusercontent.com/53431568/139845144-81277f86-5eef-4a41-85a1-572ee9745962.png)

네트워크 구조는 다음과 같이 두 개의 모듈로 이루어져 있습니다.

#### 1) Feature embedding module

입력값으로 들어온 비디오 프레임들을 CNN 컨볼루션을 통해 각 특징점 벡터를 추출합니다.

#### 2) Frame attention module

추출된 feature vector (특징점 벡터)들을 연산하여 attention weight를 계산하여 프레임에 대한 확률값을 얻어 classification을 수행한다.

attention module을 자세히 살펴보면,

![image](https://user-images.githubusercontent.com/53431568/139846857-1a391cdb-3c77-41dc-be34-75ed2608e3c2.png)

먼저 노란색으로 하이라이트된 부분에서 self-attention weight 와 global representation 값을 구하게 됩니다.

self-attention wieght는 attention weight를 구하는 방식과 동일하게 이루어지는데요, attention weight는 input의 hidden state 벡터와 ouput 에서 나올 것이라 예상되는
한 시점의 state 의 벡터 값을 각각 dot product (내적) 시켜서 output으로 나오게 될 값에 대해 모든 input과의 유사도를 구하는 방식입니다.

자연어 처리와 같은 Sequence-to-sequence 의 attention score를 얻는 구조는 다음과 같습니다.

![image](https://user-images.githubusercontent.com/53431568/139848061-ea286f8d-cfaa-4b2f-b6b6-23e9ac48cd4a.png)


이것과 유사하게 FAN에서도 feature vector 값을 input의 hidden state, FC 레이어의 파라미터 값을 출력으로 나올 것이라 예상되는 값으로 둔 것 같습니다.

여기서 $q^0$ 를 FC 레이어의 파라미터 값이라고 했는데, 실제로 공식 github 에 가서 코드를 뜯어본 결과 FC 레이어 자체를 dot product 하는 것 같았습니다. [Attention machanism 을 사용하는 다른 FER 논문](https://chaelin0722.github.io/fer/Audio_video_fer/)에서도 확인 결과 FC 레이어 자체를 내적한다고 설명했으므로, 이 논문에서도 FC 레이어 자체를 내적하는 것이라고 생각하겠습니다.

(생각해보니.. 다음에 나오게 될 것이라고 예상되는 모든 값들과의 유사도를 진행해서 어떤것이 높은 값으로 나올지를 계산하는 것이므로 FC레이어 자체로 연산하는 것이 맞는것 같습니다.)


그 다음은 attention module에서 빨간색 부분에 대한 설명입니다. 이 부분은 relation attention weight를 구하는데요, relation attention 이라는 개념은 global feature와
local feature 둘 모두를 갖고 학습하는 것이 더 좋은 성능을 낼 것이라는 가정 하에 만들어진 개념입니다.

그래서 보면, 일단 첫번째 단에서 나온 feature 값과 self-attention weight를 통해 나온 global representation 값 ($f\prime_v$) 를 concat 시켜서 이를 다시 FC레이어와 내적시켜 시그모이드 
함수를 취해  weight 값을 구하게 됩니다. 

따라서 구한 self-attention weight ($\alpha_i$)와 global representation ($\beta_i$) 를 가지고 최종 representation 값인 $f_v$를 구할 수 있게 됩니다.

![image](https://user-images.githubusercontent.com/53431568/139848667-8fbbfde8-66b8-403a-906b-64558d2e69f3.png)


<br>

## Compare relation attention vs self attention 

![image](https://user-images.githubusercontent.com/53431568/139850179-ed478368-85cc-4dc8-bafd-d461f57bfc53.png)

self attention 과 relation attention 을 비교하면 둘 모두 각 시퀀스 프레임들 중 확실한 표정을 보이는 것에 더 많은 가중치를 주는 것을 보이고 있습니다.

거기에 relation attention 이 좀 더 좋은 가중치를 주는 것을 확인했습니다.

<br>

## Experiment Result

#### - CK+
 
 먼저, CK+ 데이터셋에서의 성능을 살펴보면.. baseline 모델보다 attention 을 취한 모델이 더 좋은 성능을, relation attention 을 사용한 모델이 조금 더 좋은 성능을 내는 것을
 확인할 수 있습니다. 여기서 CK+로 나온 성능은 SOTA 성능을 보인다고 합니다!
 
 
![image](https://user-images.githubusercontent.com/53431568/139851794-587f778e-303e-4e4c-9d20-5e3ccdc6b5af.png)




#### - AFEW 8.0 

영화 데이터로 이루어진 AFEW 8.0 데이터셋에서의 성능은 다음과 같습니다. SOTA 성능까지는 아니지만 그래도 relation attention을 사용할 때의 성능이 더 좋게 나온다는 것을 확인할 수 있습니다.

![image](https://user-images.githubusercontent.com/53431568/139852137-c542a8c7-8a40-4d94-8031-b761cb446fa2.png)



