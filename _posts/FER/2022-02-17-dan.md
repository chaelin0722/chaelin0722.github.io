---
title:  "[논문정리📃] Distract Your Attention: Multi-head Cross Attention Network for Facial Expression Recognition"
excerpt: "-DAN-"

categories:
  - fer
  
tags: [fer,CNN, paperReview]
use_math: true

last_modified_at: 2022-02-17T08:06:00-05:00
classes: wide
---

## Noisy Student Training using Body Language Dataset Improves Facial Expression Recognition
#### -DAN-

[논문원본](https://arxiv.org/pdf/2109.07270.pdf)😙 

이번엔, Papers with code 기준, AffectNet Data로 현재 2위를 차지하고 있는 논문을 분석해보고자 한다.

FER에 meta learning 기법을 적용하고자 여러 meta learning 기법을 공부하던 중, 기존 방식들은 완전 다르게 생긴 이미지에서 각 클래스 당 적은 데이터만을 가지고 연산하는 알고리즘 방식을 제시한다.

하지만 얼굴 데이터의 경우, 얼굴이라는 범주는 같고, 그 안에서 미세한 표정차이만 있기 때문에 그 차이에 초점을 맞추면서 알고리즘을 어떻게 접근해야할 지에 대해 고민하고 있다.

그래서 약간 거리 기반을 이용하면서 얼굴 부분, 중요한 표정이 변화하는 그 부분에 초점을 맞춘 논문을 찾아서 공부해 보았다.


논문에서 제시하고 있는 전체 모델 구조는 아래와 같다.

![image](https://user-images.githubusercontent.com/53431568/154460463-ed3f5940-c778-4fd4-84c2-eeaaa0cabab3.png)

여느 모델들과 비슷하게 base model (여기서는 resnet 사용) 으로 feature을 추출하여 그 feature로 attention을 거쳐서 처리하는 방식이다.

크게 FCN, MAN, AFN 세 모듈로 구성되어있다.

<br>

### - FCN(Feature Clustering Network)

![image](https://user-images.githubusercontent.com/53431568/154460989-53e5e913-5acf-4a51-ae61-659cbe001c0a.png)


<br>

#### Affinity Loss

수식은 아래와 같으며, $M = Y$, 여기서 Y는 클래스(7개), 이며 feature값 $x^'$ 에 각 class에 해당하는 random으로 정해진 center point의 차이로 loss를 구하고 있다.

![image](https://user-images.githubusercontent.com/53431568/154461199-8564a5c2-1e23-4dd0-bfb1-b979897bb296.png)


아래는 Affinity Loss 를 사용함으로써 inter class 간의 거리를 최대화 시키면서 intra class의 거리는 최소화 시키는 식으로 데이터 분포가 정리되는 것을 볼 수 있다.

![image](https://user-images.githubusercontent.com/53431568/154461235-33a152c4-cebb-462a-9c9b-86078a4915f0.png)

<br>

### - MAN (Multi-head cross Attention Network)

Attention 모듈은 총 2가지를 사용하고 있으며 Spatial, Channel attention을 사용하고 있다. 두 attention은 이전 논문에서도 다뤘는데 이름만 같고 그 내용은 다른 것 같다.

SA와 CA 를 사용한 논문은 [[논문정리📃] Noisy Student Training using Body Language Dataset Improves Facial Expression Recognition](https://chaelin0722.github.io/fer/noisystudent/)
여기를 참고하면 될 것 같다.


![image](https://user-images.githubusercontent.com/53431568/154462978-ddc7406a-bc65-4f8f-9a29-76405f1ef5b8.png)




![image](https://user-images.githubusercontent.com/53431568/154463215-a96d41cb-cb80-4dd4-839e-3f9b3198dfa3.png)


이번 attention 에서는 local feature을 더 잘 뽑기 위해서 3x3, 1x3, 3x1 컨볼루션을 각각 거쳐서 summation 하는 방식으로 사용하고 있다. 당연한 말이겠지만 여러 사이즈의 kernel로 convolution을 하고 더하면
특징 feature들이 더해져서 중요 부분의 값이 더 커지기 때문에 지역적 특징을 잘 포착할 수 있게 된다.

이 후, spatial attention의 output 값은 Channel Attention의  input 값으로 들어가서 두번의 linear를 거치게 된다.



<br>

### - AFN (Attention Fusion Network)

![image](https://user-images.githubusercontent.com/53431568/154471553-268c347d-2d2a-49b1-9624-ed862a23abda.png)

MAN 모듈을 거쳐 나온 피쳐들을 시그모이드함수를 최한 후 각각 summation을 해주어  linear -> batch normalization을 거쳐서 최종 feature를 뽑게 된다.


#### Partition Loss

AFN 에서 사용하는 loss는 다음과 같은데, 위에서  attention 이 k 번 돌아가므로 그것 만큼 표준편차로 나누어서 log softmax로 계산하고 있다.

![image](https://user-images.githubusercontent.com/53431568/154471570-6916a3e9-a6e7-4d91-824d-67762a7d9860.png)


결국 최종 loss 는 3가지 loss를(affinity + partition + ce) 모두 합하여 업데이트 하는데, 앞에 파라미터를 곱해서 가중치를 주어 계산해준다 (논문에서는 1.0 으로 설정해줌)

![image](https://user-images.githubusercontent.com/53431568/154471796-2174e83d-e97f-43ef-8475-f5911182d514.png)

<br>

### Performance

![image](https://user-images.githubusercontent.com/53431568/154471915-b0ec4b78-4547-482e-be05-121f956f1a67.png)


아래 그래프는 attention을 몇번 돌렸을 때 성능이 좋은지를 판단하기 위해 그린 표이다. 4번을 반복할때의 성능이 가장 좋았고, 오른쪽 그림에서는 각각 attention이 돌아가면서 feature을 visualization한 결과이다.
눈과 입주변에 중요도가 분포된 것을 확인할 수 있다.

![image](https://user-images.githubusercontent.com/53431568/154471951-5315476d-124b-44cc-a3ad-dc913e63fa49.png)

4번반복하는게 가장 좋은 성능이라는데 어찌보면 당연한 결과인 것 같다 양쪽 눈과 양쪽 입가가 표정 변화에 있어서 가장 중요한 부분이자 클래스들을 구분할 수 있는 지표이기 때문에 결과가 그렇게 나온 것 같다.

만약 3번 반복했다면 입술이나 눈 한쪽에 대한 지표가 누락되어있을 텐데, 표정의 대칭이 중요하므로(의미심장한 표정을 지을 때 한쪽 눈만 커지거나 그러면 한쪽 눈에 대해서만 분류한다면 결과는 다르게 나오겠죠?)
정확도의 차이가 확실히 나는 것 같습니다.




이제 슬슬 주제를 구체화 하고 실험을 깔짝깔짝~ 해볼 시기가 왔다.. 일단 AFEW 데이터 셋을 잘 분석해보면서 Meta learning 공부해야겠다. 화이팅!!😁😁



