---
title:  "[논문정리📃] Facial Expression Recognition with Visual Transformers and Attentional Selective Fusion"
excerpt: "-VTFF-"

categories:
  - fer
tags: [fer,CNN, paperReview]
use_math: true

last_modified_at: 2021-11-30T08:06:00-05:00
classes: wide
---

## Facial Expression Recognition with Visual Transformers and Attentional Selective Fusion
#### - VTFF - 

[논문원본](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9585378)😙, IEEE Transactions on Affective Computing 에 실린 논문으로 2021년 10월에 등재.

#### 이 논문은 처음으로 FER 분야에서 Transformer을 사용하여 RAF-DB, FERPlus and AffectNet 데이터셋에 대해서 SOTA 성능을 달성

이 논문의 메인 CONTRIBUTIONS 를 살펴보면,

1) Visual Transformer을 feature fusion 을 사용하는 방식을 제안하였고, LBP 피쳐와 CNN 피쳐를 통합시켜 global-local attention 값을 도출하였고 이를 통해 FER 성능을 향상시켰다는 점에서 의의가 있음

2) ASF(Attention Seletive Fusion) 이라는 모듈을 정의하였고 글로벌한 얼굴 정보와 로컬 얼굴 정보에 대해 합쳐서 사용하는 모듈이다. 이를 통해서 필요없는 정보는 압축하는 식으로 종단간 학습을 할 수 있다.

3) 처음으로 FER 분야에서 Transformer을 사용하였다. global self-attention은 전체적인 네트워크가 visual feature sequences의 요소들사이의 관계를 학습할 수 있게 하였고 정보의 결핍이 있는 부분은 무시할 수 있도록 하였다.

4) RAF-DB, FERPlus and AffectNet 에서 기존 SOTA 모델 보다 좋은 성능을 보인다.

<br>

### LBP (Local Binary Pattern)

논문 내용에 들어가기 앞서, 이 논문에서는 RGB 이미지와 동시에 LBP 라는 이미지도 함께 학습시켜 사용한다. LBP 이미지란, 이미지를 binary 값을 사용하여 특징을 추출한 어떤 이미지로 변환시키는 기법인데, 
아래 이미지를 보고 이해해보자! 😸

![image](https://user-images.githubusercontent.com/53431568/144010536-da878871-fd8e-4999-8779-6ab0f2810653.png)

[이미지 출처: https://towardsdatascience.com/face-recognition-how-lbph-works-90ec258c3d6b]

먼저 이미지를 그레이 스케일로 변환 한 후에, 모든 픽셀에 대해 sliding window 를 하면서 3x3 pixel 값으로 가져와서 가장 가운데의 값을 기준으로 나머지 8개의 영역의 숫자와 비교를한다.
여기서 가운데 숫자보다 크면 1, 작으면 0 으로 이진화를 시켜준 후, 각 값들을 일렬로 나열하여 2진수로 나타낸 값을 다시 10진법으로 바꿔준다.

예시 사진을 보면 가운데 값인 90을 기준으로 작은 것은 0, 큰 값은 1로 변환 한 후에, 왼쪽에서 오른쪽 위에서 아래 순서대로 concatenate 한 값을 다시 10진법으로 바꾸어서 가운데 값을 141로
변환해 주었다.

**이런식으로 모든 픽셀에대한 값들을 변환해 주어 `원본 이미지보다 이미지의 특징을 잘 나타낼 수 있는 이미지로 변환`되는 것이다.

<br>

### Overview of proposed VTFF (Visual Transformer Fusion Feature)

전체적은 모델 구조는 다음과 같다.

![image](https://user-images.githubusercontent.com/53431568/144011232-b6267b5a-45fb-42ce-b20f-6cf2fbd7e943.png)

RGB 이미지와 LBP 이미지를 각 같은 feature extract 모델을 통과시켜 나온 피쳐맵(feature map) 에 대해 Attentional Selective Fusion 모듈을 거친 후, 이를 다시 1D 로 만들어 Transformer 구조에 통과시켜 나온 값들을 
가지고 emotion에 대한 classification을 진행해준다. 

후하후하😁 뭔가 장황한데 하나씩 살펴보자


<br>

### VTFF (Feature extract backbone)

전체 구조 중, input과 feature extract 부분이다.

![image](https://user-images.githubusercontent.com/53431568/144012140-619a9e9f-896b-423f-9e98-734361c63c15.png)

먼저 3채널을 가진 RGB 이미지로 어떻게 LBP를 만드는지 살펴보면, RGB의 각 채널에 대한 LBP 이미지를 생성해준 후 각 이미지들을 Concatenate 시켜서 같은 3채널의 차원으로 만들어준다.
이렇게 되면 Feature extract backbone 인 각각의 renset18에 같은 input 사이즈로 들어가게 된다. 이때, ResNet18 은 MS-Celeb-1M 데이터 셋으로 pre-trained 된 weight로 weight를 초기화 시켜준다.  

![image](https://user-images.githubusercontent.com/53431568/144011701-a4364518-4667-4056-ada8-5a9c28213ce9.png)


두 이미지가 각각의 resnet18을 통해서 피쳐맵을 얻게 되면, 이것을 다음과 같이 $X_{RGB}$, $X_{LBP}$ 라고 한다. 각 차원은 아래와 같이 R(논문에서는 32)의 비율만큼 차원이 축소되며 
$H /over R$ 과 같은 분수롤 $H_d$ 와 같이 재 정의 해주었다고 한다.

![image](https://user-images.githubusercontent.com/53431568/144012360-2e57027b-1bbd-4edb-9f31-01954f4eaea5.png)

<br>

### VTFF (Attentional selective fusion module = ASF)

다음은 어텐션 모듈이다, 위에서 도출한 feature 값에 대해서 attention 연산을 진행해주게 되는데, 먼저 두 feature값을 integration의 최기 wegigt 값을 곱해준 후 더해준다.

이것을 U 라고 정의한다. 그럼 이제 이 U 를 각각 Local Attention, Global Attention을 진행해 주게 되는데 그 구조에 차이가 있다.

- `Local Attention` 의 경우 input feature에 대한 미세한 디테일들을 기억하고 있으며, 컨볼루션을 거치면서 Height와 Width는 그대로, channel 값은 정해진 값으로 줄어들게 되면서
차원의 공간은 input size 만큼 그대로 유지하면서 점점 채널의 차원이 줄어들게 된다. 

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ==> 이것으로 지역적으로 흩어져 있는 차별적 정보에 집중하게 된다

- 반면, `Global Attention`의 경우 채널 차원은 그대로 둔 채, Height와 Width의 값을 1로 줄인다.

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ==> 이것으로 채널 차원 안에서 feature들의 글로벌한 관계를 포착할 수 있도록 한다.

*서로가 서로의 보완점이 될 수 있는 값들을 도출하게 된다.

아래 수식을 참고하자!

![image](https://user-images.githubusercontent.com/53431568/144014915-1150ff38-7af6-4227-9831-de137edd46e7.png)


어텐션 모듈의 마지막 부분이다..! 위에서 구한 $G(U)$와 $L(U)$를 broadcasting addition을 통해 더한 값을 $GL(U)$ 라고 재정의 해준다.

여기서 Broadcasting 에 대해 살짝 언급하자면, 차원이 달라도 계산이 가능해지도록 자동으로 맞춰 계산할 수 있는 기법이다.

아래 이미지 참고!

![image](https://user-images.githubusercontent.com/53431568/144016011-66c3c1cd-b515-4541-92cc-2c1d3836359b.png)


다시 어텐션 모듈로 돌아와서, 이렇게 구한 $GL(U)$ 를 다시 모듈의 입력단에 들어왔던 피쳐맵과 elementwise multiplication을 통해 계산을 해주고 또 다시 더해준 값을 feature fusion된 값 
$X_{fused}$ 라고 정의해준다. 

여기서 LBP 의 피쳐맵에는 $GL(U)$을 그대로 계산해주고, RGB 의 피쳐맵에는 $1-GL(U)$ 을 곱해주는데, 여기서  $1-GL(U)$은 1값을 가진 matrix 로, 모든 $GL(U)$의 element에서 1씩 빼준다고 
생각하면 된다. 두 피쳐맵에 다르게 곱해주는 이유는, 어차피 attention layer의 입력에 들어온 값이 LBP와 RGB의 특징들이 잘 어우러진 값들이 들어올 것이고, 이를 통해 얻은 값 또한, 
특징을 반영하고 있을 거기 때문에 입력의 feature map 에도 비율을 다르게 해서 계산해주는 것이라고 판단된다.

![image](https://user-images.githubusercontent.com/53431568/144016082-b687e087-bf1d-43aa-97c6-67c465ef79de.png)


<br>

### VTFF (Multi-Layer Transformer Encoder)

다음은 ASF에서 얻은 matrix 를 1차원으로 변경시켜서 시퀀스의 맨 앞에 cls 토큰을 붙이고~ 진행시켜 주는데, 이는 [VIT 논문 리뷰에서 자세히 다루었으니 여기 참고!](https://chaelin0722.github.io/paperreview/ViT/)

![image](https://user-images.githubusercontent.com/53431568/144017798-bc828e41-6380-4b95-b261-560dd22dc475.png)


<br>
<br>

### Results

다음은 이전 SOTA 모델들과의 성능을 비교한 표이다.

![image](https://user-images.githubusercontent.com/53431568/144018235-23b176b0-7dec-49dd-b160-6d3832dd3f25.png)



<br>

<br>


#### 참고

[1] [Local Binary Pattern](https://towardsdatascience.com/face-recognition-how-lbph-works-90ec258c3d6b)
