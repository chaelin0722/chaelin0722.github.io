---
title:  "[Paper ReviewðŸ“ƒ] Facial Expression Recognition in the Wild via Deep Attentive Center Loss"
excerpt: "-DACL-"

categories:
  - fer
  - centerLoss
  
tags: [fer,CNN, paperReview]
use_math: true

last_modified_at: 2022-03-08T08:06:00-05:00
classes: wide
---

## Facial Expression Recognition in the Wild via Deep Attentive Center Loss
#### -DACL-

[Paper](https://arxiv.org/pdf/2109.07270.pdf)ðŸ˜™ 

Studying 
I searched FER using center loss. This model is a SOTA model, ranking each 6 and 5 in RAF-DB and AffectNet as of today (09 March 2022). 


## Intro

- In FER, soft max loss was widely used. However, softmax loss is incapable of yielding discriminative features in wild scenarios. 

- Deep Metric Learning(DML) approaches constrain the embedding space to obtain well-discriminated deep features.
- In a typical DML problem, the deep feature equally contributes to the DMLâ€™s objective function along all dimensions. Therefore, DML methods are prone to discriminate redundant and noisy information along with important information encoded in the deep feature vector. This leads to over-fitting and hinders the generalization ability of the learning algorithm


âžœ Paper designed a modular attention-based DML approach, called `Deep Attentive Center Loss (DACL)`, to selectively learn to discriminate exclusively the relevant information in the embedding space. 

![image](https://user-images.githubusercontent.com/53431568/157199478-25e1dde1-f7e1-4596-a925-8e6ad8e2b32e.png)

DACL extract attention weights to apply it in loss computational process.


<br>

## DACL method

![image](https://user-images.githubusercontent.com/53431568/157432748-ded966df-0413-428b-8abe-6b658aca43a4.png)

Above image shows a whole process of the proposed model. When input image goes to CNN(ResNet18), the last layer's feature goes to two different ways.

The DACL take flattened feature as a input for attention network. The output of attention network will be attention weights which will be element-wise multiplicated with sparse center loss computation.

the same last layer's feature go through pooling layer, will be computed in sparse center loss and softmas loss each. 

The final loss will be summation of softmax loss and sparse center loss.

<br>

## DACL method
### - Context Encoder (CE) Unit

![image](https://user-images.githubusercontent.com/53431568/157457325-7930a36c-1b8e-4487-8b27-46719d91a7c4.png)


The three fully connected layer can be mathematically notated to...


<img src="https://user-images.githubusercontent.com/53431568/157457665-6f41f298-b92a-4c83-b68c-2dc89bef7e26.png" width="300" height="80"/>

Since CE Unit is composed of FC layer, the significant features can be extracted well. The final single unit vector $e_i$ is a latent representation vector.

<br>

## DACL method
### - Multi-head binary classification

![image](https://user-images.githubusercontent.com/53431568/157457908-b467b6ba-36d5-4a18-8d13-e7c81eb66868.png)

<br>

<img src="https://user-images.githubusercontent.com/53431568/157457925-d696f4ca-73bb-4af0-a54e-ba6d7e6bd272.png" width="250" height="80"/>
<img src="https://user-images.githubusercontent.com/53431568/157463484-ed8b7056-e72f-433b-9273-12154ca9ce6f.png" width="300" height="80"/>

attention value $a_{ij}$ eventually saturates 0~1.


<br>

## DACL method
### - sparse center loss

![image](https://user-images.githubusercontent.com/53431568/157461957-3a1eda6d-8ce2-49ab-a5ca-cac8bbf954e4.png)

<img src="https://user-images.githubusercontent.com/53431568/157461970-b5ac8fc5-37be-4ac6-b29a-262c5a1ce24e.png" width="400" height="130"/>

as you can see, the difference between feature and center point process element-wise multiplication with attention weight.




<br>

## Training DACL

1. Sparse center loss is jointly optimized with softmax loss : $L = L_S$ &nbsp; $+ \lambda L_{SC}$


2. Sparse center loss contributes to the gradients with respect to the deep features and their corresponding attention weights

<img src="https://user-images.githubusercontent.com/53431568/157462497-a5e9b3ef-9fab-4c41-9a16-0d25dcd0baa3.png" width="250" height="170"/>


3. Centers are updated using a moving average strategy

<img src="https://user-images.githubusercontent.com/53431568/157462536-f2ab2e48-1627-4538-a300-d79bfdb97033.png" width="350" height="100"/>



<br>

## Experiment
#### RAF-DB


<img src="https://user-images.githubusercontent.com/53431568/157464395-0872e77b-3f8f-4ff8-83a9-1dc344cf0f0b.png" width="300" height="350"/>

![image](https://user-images.githubusercontent.com/53431568/157464405-2dd79f24-dfda-4d51-b9cc-27daf14ff30f.png)

<br>

#### AffectNet


<img src="https://user-images.githubusercontent.com/53431568/157464503-4d99a5f6-30d3-426f-a446-53321ad45366.png" width="300" height="350"/>

![image](https://user-images.githubusercontent.com/53431568/157464447-a5d1b9d3-5b02-4956-9033-ca57ed65ebb1.png)


### Attention weights visualization

<img src="https://user-images.githubusercontent.com/53431568/157464522-40ebdef4-aa6e-4f31-9391-17b888d9c735.png" width="450" height="400"/>
