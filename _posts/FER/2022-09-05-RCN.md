---
title:  "[Paper Review📃] Convolutional relation network for facial expression recognition in the wild with few-shot learning"
excerpt: ""

categories:
  - FER
  - Attention
  - fsl
  - Relation Network
tags: [Deeplearning, FER]

last_modified_at: 2022-09-05T08:06:00-05:00
---

오늘 리뷰할 논문은 ➡️ Zhu, Qing, et al. "Convolutional relation network for facial expression recognition in the wild with few-shot learning." Expert Systems with Applications 189 (2022): 116046.

FER의 성능을 끌어올릴 수 있는 새로운 metric 방식을 연구하여 few-shot learning의 다른 method들과 비교한 논문이다. FER분야에 맞춤형 metric을 선보였다는 것에서 의의가 있는데 내용은 조금 부실해서 아쉬웠던 논문이다. 

하지만 FER 분야를 다른 학습방법으로 적용시켜 연구하고 싶은 나에게 있어 매우 단비같은 그런 논문이랄까 ㅎㅎ


### Framework overview

먼저, 이 논문에서 주장하는 모델의 아키텍쳐 전반을 살펴보자. 

![image](https://user-images.githubusercontent.com/53431568/188362162-1a5700fb-98ef-4e45-988c-f98984f10cb4.png)

few-shot learning과 같이 support set과 query set이 input으로 들어가고 1) feature embedding 을 거쳐 나온 feature를 한면 2) depth attention pooling을 거친 후 support feature와 query feature의 concate을 시행하여, 3) 마지막 convolution layers들을 거쳐 concatenation된 feature 값과 2)를 거치지 않고 나온 feature 값을 곱하여 사용한다. 

이 단계들이 의미하는 바가 무엇인지 하나씩 알아보자!

### Stage1 : Feature Embedding

![image](https://user-images.githubusercontent.com/53431568/188363016-7987db8b-8242-4bf0-ae4a-48f65a9ff233.png)

첫번째로는 input으로 들어오는 support set 이미지들과 query set 이미지들에 대하여 각각 feature를 뽑는 것이다. 이 모델자체는 relation network로 구성되어있으며, 첫번째 단계에 해당하는 layer는 4개의 convolution block으로, 아래와 같이 구성되어있다. 

- Relation network convolution
> Each convolution block has
> 
> 3x3 convolution of 64 filters
> 
> Batch normalization
> 
> Relu activation function layer
> 
> 2x2 max pooling

feature embedding 부분의 식을 $f_\theta$ 라고 했을 때, 

- feature map of support set : $f_\theta(S^{(i)})$
- feature map of query set : $f_\theta(Q^{(j)})$ 


### Stage2 : Salient Discriminative Feature Learning

#### 1) Depth Average Pooling (DAP)

![image](https://user-images.githubusercontent.com/53431568/188363678-97cd3465-da53-439a-a6c7-f90d21a546b6.png)

이 부분에 대해서 해석이 좀 어려웠는데, depth average pooling 하니까 당연히 channel attention이겠거니~ 생각을 했는데, 진작에 channel attention이었다면 Global Average Pooling이라는 개념이 있는데, 그 단어를 썼겠지! 라는 생각이 들어 다시 논문을 꼼꼼하게 읽었다. 읽어보니, channel attention은 아니고, 레이블이 같은 support set 이미지들이 여러개가 들어오게 되는데, 같은 레이블에 해당하는 이미지들에 대해서 DAP를 해주는 것이었다! 그래서 한 레이블에 대한 여러 support set 이미지들은 하나의 feature map으로 pooling이 된다. 구체적으로 써있진 않지만, 논문에서 제시하는 수식과, 글에 근거하면 이렇게 되는 것이 맞을 것이다.

> 또한, DAP 는 support set 이미지들에만 해주는데 그 이유는, 이 풀링을 거치면 각 이미지들이 가진 `"commonality"`를 뽑고, 유사하지 않은 다른 부분에 대해서는 정보를 없앨 수 있었다고 한다. 


#### 2)







