---
title:  "[Paper Review📃] Convolutional relation network for facial expression recognition in the wild with few-shot learning"
excerpt: ""

categories:
  - FER
  - Attention
  - fsl
  - Relation Network
tags: [Deeplearning, FER]

last_modified_at: 2022-09-05T08:06:00-05:00
---

오늘 리뷰할 논문은 ➡️ Zhu, Qing, et al. "Convolutional relation network for facial expression recognition in the wild with few-shot learning." Expert Systems with Applications 189 (2022): 116046.

FER의 성능을 끌어올릴 수 있는 새로운 metric 방식을 연구하여 few-shot learning의 다른 method들과 비교한 논문이다. FER분야에 맞춤형 metric을 선보였다는 것에서 의의가 있는데 내용은 조금 부실해서 아쉬웠던 논문이다. 

하지만 FER 분야를 다른 학습방법으로 적용시켜 연구하고 싶은 나에게 있어 매우 단비같은 그런 논문이랄까 ㅎㅎ


### Framework overview

먼저, 이 논문에서 주장하는 모델의 아키텍쳐 전반을 살펴보자. 

![image](https://user-images.githubusercontent.com/53431568/188362162-1a5700fb-98ef-4e45-988c-f98984f10cb4.png)

few-shot learning과 같이 support set과 query set이 input으로 들어가고 1) feature embedding 을 거쳐 나온 feature를 한면 2) depth attention pooling을 거친 후 support feature와 query feature의 concate을 시행하여, 3) 마지막 convolution layers들을 거쳐 concatenation된 feature 값과 2)를 거치지 않고 나온 feature 값을 곱하여 사용한다. 

이 단계들이 의미하는 바가 무엇인지 하나씩 알아보자!

### Stage1 : Feature Embedding

![image](https://user-images.githubusercontent.com/53431568/188363016-7987db8b-8242-4bf0-ae4a-48f65a9ff233.png)

첫번째로는 input으로 들어오는 support set 이미지들과 query set 이미지들에 대하여 각각 feature를 뽑는 것이다. 이 모델자체는 relation network로 구성되어있으며, 첫번째 단계에 해당하는 layer는 4개의 convolution block으로, 아래와 같이 구성되어있다. 

- Relation network convolution
> Each convolution block has
> 
> 3x3 convolution of 64 filters
> 
> Batch normalization
> 
> Relu activation function layer
> 
> 2x2 max pooling

feature embedding 부분의 식을 $f_\theta$ 라고 했을 때, 

- feature map of support set : $f_\theta(S^{(i)})$
- feature map of query set : $f_\theta(Q^{(j)})$ 


### Stage2 : Salient Discriminative Feature Learning

![image](https://user-images.githubusercontent.com/53431568/188430852-4e4c4439-2ad3-4875-a5fe-36997b44d0ad.png)

#### 1) Depth Average Pooling (DAP)

![image](https://user-images.githubusercontent.com/53431568/188363678-97cd3465-da53-439a-a6c7-f90d21a546b6.png)

이 부분에 대해서 해석이 좀 어려웠는데, depth average pooling 하니까 당연히 channel attention이겠거니~ 생각을 했는데, 진작에 channel attention이었다면 Global Average Pooling이라는 개념이 있는데, 그 단어를 썼겠지! 라는 생각이 들어 다시 논문을 꼼꼼하게 읽었다. 읽어보니, channel attention은 아니고, 레이블이 같은 support set 이미지들이 여러개가 들어오게 되는데, 같은 레이블에 해당하는 이미지들에 대해서 DAP를 해주는 것이었다! 그래서 한 레이블에 대한 여러 support set 이미지들은 하나의 feature map으로 pooling이 된다. 구체적으로 써있진 않지만, 논문에서 제시하는 수식과, 글에 근거하면 이렇게 되는 것이 맞을 것이다. 또한, support set 들 간의 관계를 average pooling 방식으로 계산한다는 점에서 괜찮은 방법인 것 같다고 생각한다. 

> DAP 는 support set 이미지들에만 해주는데 그 이유는, 이 풀링을 거치면 각 이미지들이 가진 `"commonality"`를 뽑고, 유사하지 않은 다른 부분에 대해서는 정보를 없앨 수 있었다고 한다. 


#### 2) JS Divergence

이건 GAN 논문리뷰하고 개념 정리를 하면서 다뤘던 개념이라 ➡️ [[GAN study] KL-divergence & JS-divergence & Maximum Likelihood Estimation와 개념정리](https://chaelin0722.github.io/gan/KL_divergence&JS_divergence/) 혹은 [[논문정리📃] Generative Adversarial Nets](https://chaelin0722.github.io/paperreview/generative_adversarial_nets/) 이 포스팅을 참고하면 이해가 빠를 것 같다. 

$D_{JS}^{i,j} (P(f^a_\theta(S^{(j)})),P(f^a_\theta(S^{(j)}))) $

이 metric의 장점은..

- JS Divergence 로 loss를 계산하여 감정 이미지를 구분하는 능력을 증대시킬 수 있다. 
- 다른 클래스끼리 멀리 떨어질 수 있도록 penalize 할 수 있다.

DAP의 식
- Feature map of support set’s DAP : $f^a_\theta(S^{(j)})$

결국 stage2에서 사용하는 Loss function의 최종식은!

$L_{dist}^k = 1 - $ $1\over{N^2}$ $\sum^N_{i=1} \sum^N_{j=1} [y_k^{i,j} - D_{JS}^{i,j} (P(f^a_\theta(S^{(j)})),P(f^a_\theta(S^{(j)})))]^2$

### Stage3 : Emotion Similarity Learning

![image](https://user-images.githubusercontent.com/53431568/188430779-e559efa9-a541-4b42-844a-6b051987e12c.png)

이 단계에서는, 바로 전 단계에서 구한 DAP 를 거친 support set feature 과, query set feature 을 concatenation 을 시켜 relation network의 나머지 4개의 layer을 거치게 한다.

또한, dap 를 거치지 않은 support set과 query set의 similarity를 구하기 위해, concate 한 feature가 나온 feature와 그냥 8개의 layer을 거쳐나온 feature의 곱으로 loss를 계산해준다. 따라서, 이번 단계에서 계산하는 loss function은 다음과 같다. 

$L_r^{(k)} =$ $1\over{N^2}$ $\sum^N_{i=1} \sum^N_{j=1} [y_k^{(i,j)} - r^{i,j} (g_𝜑 [C(f^a_\theta(S^{(j)})),P(f^a_\theta(S^{(j)}))])]^2$


### 최종 CRN Loss

$L_{CRN} =$ $1\over{K}$ $\sum^K_{k=1} (L_r^k + \lambda L_{dist}^k )$


### Experiment details

이 논문에서는 각 데이터마다 emotion label 에 해당하는 이미지의 갯수가 매우 상의하다면서 imbalance 문제를 해결하기 위해, 비슷하게 가지고 있는 수의 label을 train으로, 적은 이미지수를 갖고 있는 레이블을 test로 하여서 실험을 진행했다고 한다. 

![image](https://user-images.githubusercontent.com/53431568/188431661-f9a4ade8-07f6-47ec-939f-7751b20d905b.png)

하지만 train에 이미지를 표에 나온것을 다 썼는지 아닌지는 자세히 서술하지 않아 정확한 정보는 모른다. 또한, n-shot k-way 에 대하여 n과 k 에 대한 정보도 서술되어있지 않음.. 흠



### Experiment Results

- RAF-DB

![image](https://user-images.githubusercontent.com/53431568/188431958-f961ff12-1f3a-44fb-989d-71b132e511b2.png)


- FER2013

![image](https://user-images.githubusercontent.com/53431568/188432031-e7c8e2e2-314f-4f6a-9f30-37efbf9b006a.png)


- SFEW

![image](https://user-images.githubusercontent.com/53431568/188432044-79200a43-fb20-44cb-8f0c-8fa2833d4501.png)


###  generated feature maps with different emotion categories with/without the JS 

![image](https://user-images.githubusercontent.com/53431568/188604101-f6c080b5-1a3a-42b1-8f59-6cb7dbadef00.png)

### Model Ablation 

![image](https://user-images.githubusercontent.com/53431568/188604186-fc93024f-f5dd-4b7a-9f6e-d28211ac015a.png)


