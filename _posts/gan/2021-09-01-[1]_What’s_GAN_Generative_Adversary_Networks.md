---
title:  "[GAN study] What’s GAN Generative Adversary Networks?"
excerpt: ""

categories:
  - gan

tags: [Deeplearning, study, gan]

classes: wide

last_modified_at: 2021-09-01T08:06:00-05:00

---
## 수정중
GAN - Generative Adversarial Networks Gan 이란?

갠은 초상화를 그리거나 심포니를 작곡하는 것과 같은 창조이다.

다른 딥러닝 분야와 비교했을때 창조하는 것은 어렵다. 

컴퓨터 혹은 사람이 모네 그림을 다른 그림들과 비교하는 것은 쉬운일이다. 하지만 이를 통해 지능을 이해하는 것에 한발짝 다가갈 수 있다.

게임개발에 있어서 애니메이션을 만들기 위해 많은 개발 예술가를 고용한다. 몇몇의 일은 일상적일 것이다. 

GAN 과 자동화를 적용함으로써 우리는 일상의 루틴을 반복하기 보다는 좀 더 창조적인 일에 집중할 수 있다.

Gan Series의 부분이 되는것! 이 글에선 Gan의 개념과 알고리즘을 다룰 것이다.

What does GAN do?

GAN의 중점은 scratch로부터 데이터를 생성하는 것이다(대부분은 이미지들!. 하지만 음악을 포함한 다른 영역들은 끝났다. )

그러나 GAN의 적용범위는 꽤 크다. 아래 예시와 같이 GAN은 얼룩말로부터 말을 생성해 낸다. 강화학습을 통해 로봇이 더 빨리 배우는 것을 돕는다.

Generator와 discriminator

gan은 두가지 깊은 network로 구성되어있다. 하나는 generator(생성자), 다른하나는 discriminator(구분자)이다.

우린 먼저 생성자가 어떻게 학습을 어떻게 하는지 알기도 전에 이미지를 만들어 내는지를 살펴볼 것이다.

먼저, 어떤 noise z 가 있다고 가정하자. x를 input으로 해 generator G를 image x (x=G(z)) 를 만들 것이다. 맞아 마법같은 소리짘

개념적으로 z는 latent features of images generated를 표현한다. 예를 들어보자 딥러닝 classification에 있어서 우리는 모델이 학습하고 있는

feature를 제어하지 않는다. 비슷하게 GAN에서도 우리는 z의 semantic(의미론적) 뜻을 제어하지 않는다. 우리는 학습 프로세스가 알아서 학습하도록
냅둔다.

예를들어 z의 어떤 byte 값이 헤어컬러를 결정짓는지 제어하지 않는다. 이 의미를 살펴보자면 생성된 이미지를 구성하고 하고 스스로

시험해보는 것이 가장 효과적인 방법이다.

아래 이미지들은 random noise z를 이용해 GAN에 의해 생성된 이미지들이다.
---
z 의 한 특정 차원을 단계적으로 바꿀 수 있으며 그것의 의미론적 의미를 시각화할 수 있다. 

---
그렇다면 magic generator G는 과연 무엇일까?

아래 레이어 구조는 DCGAN의 구조이며 DCGAN은 가장 유명한 디자인 중 하나이다.

이 구조는 z를 upsampling 해 image x 를 생성하기 위해 다중 transposed 컨볼루션을 수행한다.

우리는 이것을 deep learning classifier의 반대라고 해석할 수 있다.

--
하지만 생성자 혼자로써는 random noise로 부터 창조하기만 할 뿐이다. 개념적으로 GAN의 구분자는 생성자에게 

어떤 이미지를 창조할 것인지에 대한 가이드를 제공해준다.

GAN의 또 다른 application을 고려해보자 바로 CycleGAN 이다. CycleGAN은 생성자를 사용해 실제 풍경을 모네 화풍의 그림으로 바꿔준다.


---

실제 이미지와 생성된 이미지를 학습함으로써 GAN은 구분자가 어떤 특징이 이미지를 더 실제같이 만드는지를 배우게 된다.

그렇게 되면 같은 구분자는 생성자에게 더 진짜 모네 그림같은 그림을 창조하라고 피드백을 주게 된다.

---
그렇다면 이제 기술적인 면을 살펴봅시다.

구분자는 실제 이미지(학습 샘플들)와 생성된 이미지를 구분해서 바라본다. 이렇게 입력으로 들어온 이미지가 실제 이미지인지 생성된 
것인지 구분한다.

출력값인 D(X) 는 Input x 가 실제인지에 대한 확률값이다. (ex P(class of input = real image))

---

우린 구분자를 deep network classifier과 같이 학습시킨다. 만약 입력값이 진짜라면 D(x) = 1 이 나온다. 만약 생성된 이미지라면 반대로 D(x) = 0 이 나온다.

이 과정을 통해 구분자는 실제 이미지에 기여하는 특징을 식별한다.

반대로, 우리의 목표는 생성자가 D(x) = 1을 도출하는 이미지를 생성하는 것이다.

따라서 역전파를 통해 생성자를 학습시킨다. 이 타켓 가치는 모두 생성자로 돌아갈 것이다. (예를 들어 구분자가 이건 진짜야! 라고 속을만한

이미지를 생성하도록 학습하는 것이다! 키포인트 구분자를 속여라! 가짜를 진짜로 믿도록!)

-----
우리는 생성자 구분자 각각의 network를 번갈아가며 학습시키고 그들 스스로 개선될 수 있도록 경쟁을 시킨다. 

결국 구분자는 실제와 생성된 것 사이의 아주 미세한 차이도 알아차릴 수 있게 되고, 생성자는 구분자가 이 둘 사이의 차이를 못 찾아내는

이미지를 창조해낸다. (마치 라이크 창과 방패의 싸움이랄까.. 킄)

GAN 모델은 결국 수렴하고 자연스러운 이미지를 생성하게 된다.

이 구분자 개념은 많은 현존하는 딥러닝 application에 적용되고 있따. GAN의 구분자는 비평가 역할을 한다.

우리는 구분자로 하여금 현존하는 딥러닝 해결책을 제시하도록 피드백을 제공해 더 나은 결과를 만들 수 있다.

--

### 역전파

이제 우리는 한가지 관문이 남았다 ㅋㅋ 바로 간단한 수식인데, 구분자의 결과 값은  D(x)에서 x는 실제 이미지를 뜻한다.


우리의 목표는 실제이미지를 실제로, 생성된 이미지를 가짜로 인식하는 기회를 최대로 하는 것이다!

예를 들어 연구한 데이터의 최대 가능도( i.e. the maximum likelihood of the observed data)

loss(손실값)을 측정하기 위해 cross-entropy를 Deep Learning: p log(q).로 사용한다. 

실제이미지 p는 1이다. (true label for real image)/ 생성된 이미지는 반대의 라벨을 붙인다.(-1)

이를 수식으로 정리하면 다음과 같다.  
---

생성자쪽 수식을 보면, objective function은 모델이 구분자를 높은 가능성으로 속이는 이미지를 생성하길 원한다.

생성자의 입장에서는 작을수록 좋기 때문이다. 왜! D가 속아야 그만큼 실제이미지처럼 잘만들었다는 거니깐!
---

"We often define GAN as a minimax game which G wants to minimize V while D wants to maximize it."

--

양쪽의 함수가 정의되었다면 이 둘은 번갈아가며 gradient descent를 학습한다.

학습방법은 다음과 같다.

먼저, 생성자 모델의 파라미터를 고정시키고 실제와 생성된 이미지를 갖고 구분자를 gradient dscent의 반벅을 한번 수행한다.

그다음으로 바꾸어서, 구분자를 고정하고 생성자를 한번 박복으로 학습시킨다. 두 네트워크를 번갈아가며 학습하는데 학습은 생성자가

양질의 이미지를 생성할때까지 학습된다. 아래는 데이터 흐름과 역전파를 위해 사용된 기울기를 요약한 것이다.

---

다음은 GAN이 어떻게 수행되는지 한번에 보여주는 수도코드(의사코드)이다.

---

생성자가 기울기를 없앤다?!

하지만, 우리는 기울기가 없어지는 문제를 직면하게 된다.

구분자는 생성자에 대항해 일찍 이기곤하는데(?) 이것은 이른 학습에 있어서 생성된 이미지로부터 실제 이미지를 구분하는 것이 쉽다.

V가 0에 도달할 수 있도록 만든다. ((log(1-D(G(z))) -> 0)) 

생성자를 위한 기울기는   점점 0에 수렵해 없어질 것이고 최적화를 매우 느리게 만든다. 이를 개선하기 위해 GAN에서는 

생성자에 기울기를 역전파하게 하는 반대 함수를 제공한다.



--ㅡ
More thoughs

일반적으로 데이터를 생성한다는 개념은 엄청난 잠재력을 이끌지만 운나쁘게도 엄청나게 위험하다. 

GAN 말고도 많은 생성 모델들이 있다. OpenAI의 GPT-2 는 실제 저널리스트가 쓴 것같은 문장을 생성한다. 게다가 OpenAI는

오용을 위해 데이터셋과 학습된 모델을 오픈하지 않기로 한다.

오용 남용, 등의 문제와 악용의 문제가 있다. 항상 연구윤리를 준수하면서 연구해야 할 것을 다짐하곤 한다.


