---
title:  "[논문정리📃] An Image is Worth 16X16 Words : Transformers for Image Recognition at Scale"
excerpt: "-vit-"

categories:
  - paperReview
tags: [CNN, paperReview]
use_math: true

last_modified_at: 2021-10-01T08:06:00-05:00
classes: wide
---

## An Image is Worth 16X16 Words : Transformers for Image Recognition at Scale
#### -ViT- 

[논문원본](https://arxiv.org/pdf/2010.11929.pdf)😙

오늘은 Vision Transformer에 대해서 리뷰해보도록 하겠습니다. Vision Transformer, 줄여서 ViT는 원래 자연어 처리(NLP)에서 사용하던 모델을 Vision 쪽에 적용한 모델입니다.

`transformer` 과 `attention` 에 대한 개념이 나오는데, [[논문정리📃]Attention is all you need ](https://chaelin0722.github.io/paperreview/attention/)를 먼저 읽어보면 Vision transformer에 대해 이해하기 수월 할 것입니다. :) 


[Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf) 논문 링크도 걸어두겠습니다!


## 🌕 Abstract


NLP 에서 100B 의 파라미터가 넘는 사이즈를 학습할 수 있게 된 반면, 컴퓨터 비전쪽에서는 아직 CNN 구조가 주를 이루고 있습니다. 이런 NLP의 성공에 영향을 받아 이미지에 직접적으로 Transformer을 적용하고자 하였고, transformer을 적용하여 ViT는 엄청난 크기의 데이터셋으로 학습하여 SOTA를 달성하는데 성공합니다.

프로세스를 간단히 살펴보면,

  자연어에서 입력 시퀀스를 토큰으로 쪼개어 각각을 입력값으로 해서 진행하는 것 처럼 vision 쪽에서도 이미지를 패치 단위로 쪼개어서 개별적으로 입력을 넣어 처리해 진행하게 되는데


한편, transformer을 컴퓨터 비전에 적용하기에는 `inductive bias`를 갖고 있습니다. inductive bias는 CNN을 사용할 때 나타나는 문제인데, CNN은 locality 한 정보를 추출하고 
convolution연산은 translation equivariance와 locality하다는 특징을 갖고 있습니다.

> equivariance 란, 함수 입력이 바뀌면 출력 또한 바뀐다는 뜻이며, translation equivariance는 입력 위치가 변하면 출력도 마찬가지로 위치가 변한채로 나온다는 뜻입니다.
> 
> locality 란, 이미지를 구성하는 특징들은 이미지 전체가 아닌 일부 지역에 근접한 픽셀들로만 구성되고, 근접한 픽셀들끼리만 종속성을 가진다는 성질입니다.

따라서, 이미지를 패치로 쪼개고 학습하는데, 각 픽셀들의 위치와 주변 값들이 중요한 CNN에 있어서, 이 패치들의 입력이 1차원의 값으로 병렬 처리가 된다면 원하는 출력이 나올 수 없게 된다는 뜻인 것 같습니다.

하지만!, 이 논문에서는 `엄청난 양의 데이터`를 사용해 이 inductive bias를 없앨 수 있었다고 말합니다. 결국.. 데이터가 많은게 한수 였나봅니다.🐱

![image](https://user-images.githubusercontent.com/53431568/137475377-8963dd9f-ef71-4fb3-8442-3501b7cb5fa5.png)

<br>

## 🌕 Method

![image](https://user-images.githubusercontent.com/53431568/137476320-7b2d4e24-b85d-4a3d-ae88-344973634366.png)


위의 이미지와 같이 이미지를 일정한 값의 patch로 분할하고 이 patch 들을 embedding을 시켜서 transformer의 입력값으로 사용하게 됩니다. 개별의 patch를 NLP의 token 처럼 간주하는 것이죠.


프로세스를 자세히 살펴보겠습니다.

![image](https://user-images.githubusercontent.com/53431568/137476415-7611a0fb-3da0-42fb-a617-10fd83e78519.png)

먼저 48x48의 이미지를 input image로 한다면, (16x16) * 3 의 형태로 총 9개의 patch 형태로 쪼갭니다. 그리고 각각의 patch들을 linear 하게 적용시켜서 patch embedding을 진행합니다. 따라서 768차원의 하나의 벡터가 되는 것입니다.

16x16x3 = 768 = D


<br>

![image](https://user-images.githubusercontent.com/53431568/137476446-1886974b-c6e4-4d02-a44a-3c7130eba869.png)

그 다음에는 각각의 embedding된 패치에 PE(position embedding) 를 더해준 값을 최종 transformer의 입력 벡터로 사용하게 됩니다.
이때, 맨 앞에는 CLASS TOKEN 이라는 것이 붙는데요, 자연어 처리의 BERT 모델에서 나온 개념을 사용한 것입니다. class token은 BERT의 [CLS]토큰과 유사하게, 
Output vector will be used for Image representation for classification
![image](https://user-images.githubusercontent.com/53431568/137478559-a26f1fa6-32c8-43af-9fef-2a242e6676bb.png)



#### 참고

[1] [inductive bias](https://seongkyun.github.io/study/2019/10/27/cnn_stationarity/)

[2] [https://www.youtube.com/watch?v=bgsYOGhpxDc](https://www.youtube.com/watch?v=bgsYOGhpxDc)
