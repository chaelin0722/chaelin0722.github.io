---
title:  "[논문정리📃] Attention Is All You Need"
excerpt: "Week9 -Self Attention-"

categories:
  - paperReview
tags: [CNN, paperReview]
use_math: true

last_modified_at: 2021-10-05T08:06:00-05:00
classes: wide
---

## Attention Is All You Need
#### - Self Attention - 

[논문원본](https://arxiv.org/pdf/1706.03762.pdf)😙

<br>


RNN
Model architecture of Encoder-Decoder development form of RNN
Long input sequence => information loss(Long-term dependency) 



Transformer
Attention mechanism that draw global dependencies between input and output
Allows more parallelization and can reach SOTA 


