---
title:  "[논문정리📃] Attention Is All You Need"
excerpt: "Week9 -Self Attention-"

categories:
  - paperReview
tags: [CNN, paperReview]
use_math: true

last_modified_at: 2021-10-05T08:06:00-05:00
classes: wide
---

## Attention Is All You Need
#### - Self Attention - 

[논문원본](https://arxiv.org/pdf/1706.03762.pdf)😙

<br>


RNN
Model architecture of Encoder-Decoder development form of RNN
Long input sequence => information loss(Long-term dependency) 



Transformer
Attention mechanism that draw global dependencies between input and output
Allows more parallelization and can reach SOTA 


https://dgkim5360.tistory.com/entry/understanding-long-short-term-memory-lstm-kr

https://omicro03.medium.com/attention-is-all-you-need-transformer-paper-%EC%A0%95%EB%A6%AC-83066192d9ab

https://www.youtube.com/watch?v=KT58deB6oPQ

https://www.youtube.com/watch?v=mxGCEWOxfe8

https://www.youtube.com/watch?v=bgsYOGhpxDc

