---
title:  "[ë…¼ë¬¸ì •ë¦¬ğŸ“ƒ] Deep Residual Learning for Image Recognition"
excerpt: "Week5 -VGG16-"

categories:
  - CNN
  - paperReview
tags: [CNN, paperReview]
use_math: true

last_modified_at: 2021-08-04T08:06:00-05:00
classes: wide
---

## Deep Residual Learning for Image Recognition
#### - Resnet - 

[ë…¼ë¬¸ì›ë³¸](https://arxiv.org/pdf/1512.03385.pdf)ğŸ˜™

Resnet56 ì½”ë“œêµ¬í˜„ í˜ì´ì§€ =>[Resnet-56](https://chaelin0722.github.io/deeplearning/cnn/code/resnet56_cifar10_code/)

<br>

## Abstract
ì´ ë…¼ë¬¸ì—ì„œëŠ” ë” ê¹Šì€ neural networkì¼ ìˆ˜ë¡ í•™ìŠµì´ ì–´ë µë‹¤ëŠ” ê²ƒì„ ê°ì•ˆí•´ residual learning frameworkë¥¼ ì œì‹œí•œë‹¤.

residual learning frameworkëŠ” ì´ì „ì˜ ë„¤íŠ¸ì›Œí¬ë³´ë‹¤ ë” ê¹Šì€ ë„¤íŠ¸ì›Œí¬ì˜ í›ˆë ¨ì„ ìš©ì´í•˜ê²Œ í•˜ê¸° ìœ„í•œ êµ¬ì¡°ì´ë‹¤.

ë˜, ì´ ë…¼ë¬¸ì„ í†µí•´ ImageNet datasetìœ¼ë¡œ VGG netë³´ë‹¤ 8ë°° ê¹Šì§€ë§Œ ë³µì¡ì„±ì´ ë‚®ì€ ìµœëŒ€ 152ê°œì˜ layerê°€ ìˆëŠ” residual netì„ í‰ê°€í•œë‹¤.

ImageNet datasetìœ¼ë¡œ 3.57%ì˜ errorìœ¨ì„ ë³´ì´ë©° Cifar-10ì— ëŒ€í•œ ë¶„ì„ë„ ì œê³µí•œë‹¤.

<br>

<br>

## 1. Introduction

### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   - Degradation

![image](https://user-images.githubusercontent.com/53431568/128124762-2ef0fd37-6591-481b-99eb-f21ff91a5824.png)

ìµœê·¼ì—°êµ¬ì—ì„œëŠ” Layerê°€ ê¹Šì„ ìˆ˜ë¡ ì„±ëŠ¥ì´ ë” ì¢‹ì•„ì ¸ì•¼ í•œë‹¤ê³  ì£¼ì¥í•˜ì§€ë§Œ, 
ìœ„ì˜ ê·¸ë˜í”„ë¥¼ ë³¼ ë•Œ, 20-layer ë³´ë‹¤ 56-layer ëª¨ë¸ì˜ ê° train, test error rateì´ ë†’ì€ ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.

ì´ê²ƒì´ ë°”ë¡œ degradation ì´ë‹¤. (overfittingì´ ë¬¸ì œê°€ ì•„ë‹ˆë¼ modelì˜ ê¹Šì´ê°€ ê¹Šì–´ì§ì— ë”°ë¼ train-errorê°€ ë†’ì•„ì§€ëŠ” ê²ƒ)

ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë³¸ ë…¼ë¬¸ì—ì„œëŠ” residual learning ì„ ì œì•ˆí•œë‹¤.

<br>

<br>

## 2. Architecture
### - Residual learning

![image](https://user-images.githubusercontent.com/53431568/128125059-2fb5d212-0d38-4599-9a88-6e3e9b364fd6.png)

ìœ„ëŠ” Residual learningì˜ ê¸°ë³¸ êµ¬ì¡°ì´ë©° degradationì„ í•´ê²°í•˜ê¸° ìœ„í•´ ë§Œë“¤ì–´ì¡Œë‹¤.

ê¸°ì¡´ì—ëŠ” H(x)ë¥¼ ì¶œë ¥ìœ¼ë¡œ í•˜ëŠ” layerì— ëŒ€í•´ Wë¥¼ ì—…ë°ì´íŠ¸ í–ˆë‹¤ë©´, residual mappingì€ ì…ë ¥ xë¥¼ ì¶œë ¥ ê°’ì— ë”í•˜ëŠ” identity mappingì„ ìˆ˜í–‰í•´ gradientê°€ ì˜ íë¥¼ ìˆ˜ ìˆë„ë¡ ì¼ì¢…ì˜ `shortcut connection`ì´ë¼ëŠ” ë°©ë²•ì„ ì‚¬ìš©í•œë‹¤. 

ì´ shortcut connectionì€ dimensionì´ ë‹¬ë¼ì§€ëŠ” ë¶€ë¶„ì—ì„œ ìˆ˜í–‰ë˜ëŠ”ë°.. ìì„¸í•œê±´ ì•„ë˜ ê¸€ì„ ê³„ì† ì½ì–´ë³´ì!

<hr>

### - Identity mapping by shortcuts

ê¸°ë³¸ì ì¸ residual block ìˆ˜ì‹ : $F=W_{2\sigma(W_1x)}$     ($\sigma$ëŠ” ReLu ë¥¼ ëœ»í•˜ë©°, biasesëŠ” ìƒëµë˜ì—ˆë‹¤.)

residual blockì„ ì •ì˜í•˜ëŠ” ìˆ˜ì‹ì€ xì™€ Fì˜ dimensionì´ ê°™ì€ ê²½ìš°ì™€ ë‹¤ë¥¼ ê²½ìš°ê°€ ìˆë‹¤.

1. dimensionì´ ê°™ì„ ë•Œ!  $y=F(x,{W_i}) + x$   => `identity mapping`
2. dimensionì´ ë‹¤ë¥¼ ë•Œ!  $y=F(x, {W_i}) + W_sx$


## 3. ì„±ëŠ¥


















