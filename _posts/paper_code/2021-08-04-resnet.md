---
title:  "[논문정리📃] Deep Residual Learning for Image Recognition"
excerpt: "Week5 -VGG16-"

categories:
  - CNN
  - paperReview
tags: [CNN, paperReview]
use_math: true

last_modified_at: 2021-08-04T08:06:00-05:00
classes: wide
---

## Deep Residual Learning for Image Recognition
#### - Resnet - 

[논문원본](https://arxiv.org/pdf/1512.03385.pdf)😙

Resnet56 코드구현 페이지 =>[Resnet-56](https://chaelin0722.github.io/deeplearning/cnn/code/resnet56_cifar10_code/)

<br>
<hr>

## Abstract
이 논문에서는 더 깊은 neural network일 수록 학습이 어렵다는 것을 감안해 residual learning framework를 제시한다.

residual learning framework는 이전의 네트워크보다 더 깊은 네트워크의 훈련을 용이하게 하기 위한 구조이다.

또, 이 논문을 통해 ImageNet dataset으로 VGG net보다 8배 깊지만 복잡성이 낮은 최대 152개의 layer가 있는 residual net을 평가한다.

ImageNet dataset으로 3.57%의 error율을 보이며 Cifar-10에 대한 분석도 제공한다.


## 1. Introduction

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  ### - Degradation

![image](https://user-images.githubusercontent.com/53431568/128124762-2ef0fd37-6591-481b-99eb-f21ff91a5824.png)

최근연구에서는 Layer가 깊을 수록 성능이 더 좋아져야 한다고 주장하지만, 
위의 그래프를 볼 때, 20-layer 보다 56-layer 모델의 각 train, test error rate이 높은 것을 볼 수 있다.

이것이 바로 degradation 이다. (overfitting이 문제가 아니라 model의 깊이가 깊어짐에 따라 train-error가 높아지는 것)

이 문제를 해결하기 위해 본 논문에서는 residual learning 을 제안한다.
![image](https://user-images.githubusercontent.com/53431568/128124773-c9b559f5-279d-471c-bac6-15a22b1c52bd.png)



## 2. architecture


## 3. 성능
