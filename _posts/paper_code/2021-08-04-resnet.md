---
title:  "[논문정리📃] Deep Residual Learning for Image Recognition"
excerpt: "Week5 -VGG16-"

categories:
  - CNN
  - paperReview
tags: [CNN, paperReview]
use_math: true

last_modified_at: 2021-08-04T08:06:00-05:00
classes: wide
---

## Deep Residual Learning for Image Recognition
#### - Resnet - 

[논문원본](https://arxiv.org/pdf/1512.03385.pdf)😙

Resnet56 코드구현 페이지 =>[Resnet-56](https://chaelin0722.github.io/deeplearning/cnn/code/resnet56_cifar10_code/)

<br>

## Abstract
이 논문에서는 더 깊은 neural network일 수록 학습이 어렵다는 것을 감안해 residual learning framework를 제시한다.

residual learning framework는 이전의 네트워크보다 더 깊은 네트워크의 훈련을 용이하게 하기 위한 구조이다.

또, 이 논문을 통해 ImageNet dataset으로 VGG net보다 8배 깊지만 복잡성이 낮은 최대 152개의 layer가 있는 residual net을 평가한다.

ImageNet dataset으로 3.57%의 error율을 보이며 Cifar-10에 대한 분석도 제공한다.

<br>

<br>

## 1. Introduction

### &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   - Degradation

![image](https://user-images.githubusercontent.com/53431568/128124762-2ef0fd37-6591-481b-99eb-f21ff91a5824.png)

최근연구에서는 Layer가 깊을 수록 성능이 더 좋아져야 한다고 주장하지만, 
위의 그래프를 볼 때, 20-layer 보다 56-layer 모델의 각 train, test error rate이 높은 것을 볼 수 있다.

이것이 바로 degradation 이다. (overfitting이 문제가 아니라 model의 깊이가 깊어짐에 따라 train-error가 높아지는 것)

이 문제를 해결하기 위해 본 논문에서는 residual learning 을 제안한다.

<br>

<br>

## 2. Architecture
### - Residual learning

![image](https://user-images.githubusercontent.com/53431568/128125059-2fb5d212-0d38-4599-9a88-6e3e9b364fd6.png)

위는 Residual learning의 기본 구조이며 degradation을 해결하기 위해 만들어졌다.

기존에는 H(x)를 출력으로 하는 layer에 대해 W를 업데이트 했다면, residual mapping은 입력 x를 출력 값에 더하는 identity mapping을 수행해 gradient가 잘 흐를 수 있도록 일종의 `shortcut connection`이라는 방법을 사용한다. 

이 shortcut connection은 dimension이 달라지는 부분에서 수행되는데.. 자세한건 아래 글을 계속 읽어보자!

<hr>

### - Identity mapping by shortcuts

기본적인 residual block 수식 : $F=W_{2\sigma(W_1x)}$     ($\sigma$는 ReLu 를 뜻하며, biases는 생략되었다.)

residual block을 정의하는 수식은 x와 F의 dimension이 같은 경우와 다를 경우가 있다.

1. dimension이 같을 때!  $y=F(x,{W_i}) + x$   => `identity mapping`
2. dimension이 다를 때!  $y=F(x, {W_i}) + W_sx$


## 3. 성능


















